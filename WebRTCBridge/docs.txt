he term endpoint device refers to a hardware device that lies at one end of a data path that originates or terminates at an application program. Examples of audio endpoint devices are speakers, headphones, microphones, and CD players. The audio data that moves along the data path might traverse a number of software and hardware components during its journey between the application and endpoint device. Although these components are essential to the operation of the endpoint device, they tend to be invisible to users. Users are more likely to think in terms of endpoint devices that they directly manipulate rather than in terms of the devices on audio adapters that the endpoint devices plug into or in terms of the software components that process the audio streams that flow to and from these adapters.

To avoid confusion with endpoint devices, this documentation refers to a device on an audio adapter as an adapter device.

The following diagram shows how audio endpoint devices differ from adapter devices.

examples of audio endpoint devices and adapter devices

In the preceding diagram, the following are examples of endpoint devices:

Speakers
Microphone
Auxiliary input device
The following are examples of adapter devices:

Wave output device (contains digital-to-analog converter)
Output controls device (contains volume and mute controls)
Wave input device (contains analog-to-digital converter)
Input controls device (contains volume control and multiplexer)
Typically, the user interfaces of audio applications refer to audio endpoint devices, not to adapter devices. Windows Vista simplifies the design of user-friendly applications by directly supporting the endpoint device abstraction.

Some endpoint devices might permanently connect to an adapter device. For example, a computer might contain internal devices such as a CD player, a microphone, or speakers that are integrated into the system chassis. Typically, the user does not physically remove these endpoint devices.

Other endpoint devices might connect to an audio adapter through audio jacks. The user plugs in and unplugs these external devices. For example, an audio endpoint device such as an external microphone or headphones lies at one end of a cable whose other end plugs into a jack on an adapter device.

The adapter communicates with the system processor through a system bus (typically, PCI or PCI Express) or external bus (USB or IEEE 1394) that supports Plug and Play (PnP). During device enumeration, the Plug and Play manager identifies the devices in the audio adapter and registers those devices to make them available for use by the operating system and by applications.

Unlike the connection between an adapter and an external bus such as USB or the IEEE 1394 bus, the connection between an endpoint device and an adapter device does not support PnP device detection. However, some audio adapters support jack-presence detection: when a plug is inserted into or removed from a jack, the hardware generates an interrupt to notify the adapter driver of the change in the hardware configuration. The endpoint manager in Windows Vista can exploit this hardware capability to notify applications which endpoint devices are present at any time. In this way, the operation of the endpoint manager is analogous to that of the Plug and Play manager, which keeps track of the adapter devices that are present in the system.

In Windows Vista, the audio system keeps track of both endpoint devices and adapter devices. The endpoint manager registers endpoint devices and the Plug and Play manager registers adapter devices. Registering endpoint devices makes it easier for user-friendly applications to enable users to refer to the endpoint devices that users directly manipulate instead of referring to adapter devices that might be hidden inside the computer chassis. The endpoint devices that are reported by the operating system faithfully track dynamic changes in the configuration of audio hardware that has jack-presence detection. While an endpoint device remains plugged in, the system enumerates that device. When the user unplugs an endpoint device, the system ceases to enumerate it.

In earlier versions of Windows, including Windows 98, Windows Me, Windows 2000, and Windows XP, the system explicitly presents only PnP devices to applications. Thus, applications must infer the existence of endpoint devices. An operating system that lacks explicit support for endpoint devices forces client applications to do more of the work themselves. For example, an audio capture application must perform the following steps to enable capturing from an external microphone:

Enumerate all the audio capture devices (these are adapter devices) that were previously registered by the PnP manager.
After selecting a capture device, open a capture stream on the device by either calling the waveInOpen function or by using the DirectSoundCapture or DirectShow API.
Call the mixerOpen function and use the other mixerXxx functions to look for a MIXERLINE_COMPONENTTYPE_SRC_MICROPHONE line that corresponds to the capture device opened in step 2. This is an educated guess.
Unblock the data path from the microphone. If the data path includes a mute node, then the client must disable muting of the signal from the microphone. If the capture device contains a multiplexer for selecting from among several inputs, then the client must select the microphone input.
This process is error-prone because the software that performs these operations might fail if it encounters a hardware configuration that its designers did not anticipate or for which it was not tested.

In Windows Vista, which supports endpoint devices, the process of connecting to the same endpoint device is much simpler:

Select a microphone from a collection of endpoint devices.
Activate an audio-capture interface on that microphone.
The operating system does all the work necessary to identify and enable the endpoint device. For example, if the data path from the microphone includes a multiplexer, the system automatically selects the microphone input to the multiplexer.

The behavior of the audio subsystem is more reliable and deterministic if applications, instead of implementing their own endpoint-identification algorithms, can relegate the task of identifying endpoint devices to the operating system. Software vendors no longer need to verify that their endpoint-identification algorithms work correctly with all available audio hardware devices and configurations—they can simply rely on the operating system for endpoint identification. Similarly, hardware vendors no longer need to verify that every relevant client application can readily identify any endpoint device that is connected to their audio adapter—they need only to verify that the operating system can identify an endpoint device that is connected to their audio adapter.

The Windows Multimedia Device (MMDevice) API enables audio clients to discover audio endpoint devices, determine their capabilities, and create driver instances for those devices.

Header file Mmdeviceapi.h defines the interfaces in the MMDevice API.

The MMDevice API consists of several interfaces. The first of these is the IMMDeviceEnumerator interface. To access the interfaces in the MMDevice API, a client obtains a reference to the IMMDeviceEnumerator interface of a device-enumerator object by calling the CoCreateInstance function, as shown in the following code fragment:

C++

Copy
  const CLSID CLSID_MMDeviceEnumerator = __uuidof(MMDeviceEnumerator);
  const IID IID_IMMDeviceEnumerator = __uuidof(IMMDeviceEnumerator);
  hr = CoCreateInstance(
         CLSID_MMDeviceEnumerator, NULL,
         CLSCTX_ALL, IID_IMMDeviceEnumerator,
         (void**)&pEnumerator);
In the preceding code fragment, CLSID_MMDeviceEnumerator and IID_IMMDeviceEnumerator are the GUID values that are attached as attributes to the MMDeviceEnumerator class object and to the IMMDeviceEnumerator interface. The CoCreateInstance call passes these values by reference. Variable hr is of type HRESULT, and variable pEnumerator is a pointer to the IMMDeviceEnumerator interface of a device-enumerator object. IMMDeviceEnumerator provides methods for enumerating audio endpoint devices. For information about the __uuidof operator, the CoCreateInstance function, and the CLSCTX_Xxx constants, see the Windows SDK documentation.

Through the IMMDeviceEnumerator interface, the client can obtain references to the other interfaces in the MMDevice API. The MMDevice API implements the following interfaces.

Interface	Description
IMMDevice	Represents an audio device.
IMMDeviceCollection	Represents a collection of audio devices.
IMMDeviceEnumerator	Provides methods for enumerating audio devices.
IMMEndpoint	Represents an audio endpoint device.
 

In addition, clients of the MMDevice API that require notification of status changes in audio endpoint devices should implement the following interface.

Interface	Description
IMMNotificationClient	Provides notifications when an audio endpoint device is added or removed, when the state or properties of a device change, or when there is a change in the default role assigned to a device.

The first task of a client audio application is to find a suitable audio device to use. The MMDevice API lets clients discover the audio endpoint devices in the system and determine which devices are suitable for the application to use. This API enables clients to retrieve collections of the available endpoint devices and get the capabilities of each device. Header file Mmdeviceapi.h defines the interfaces in the MMDevice API.

An audio adapter might contain several devices—for example, a wave-rendering device and a wave-capture device. These are adapter devices rather than endpoint devices. As mentioned previously, adapter devices are registered by the Plug and Play manager, in contrast with endpoint devices, which are registered by the endpoint manager. Each adapter device typically supports one or more endpoint devices. A rendering endpoint device (for example, headphones) can receive a stream of audio data from a client application, and a capture endpoint device (for example, a microphone) can send an audio stream to a client application.

Before enumerating the endpoint devices in the system, the client must first call the Windows CoCreateInstance function to create a device enumerator. A device enumerator is an object with an IMMDeviceEnumerator interface. For information about CoCreateInstance, see the Windows SDK documentation.

The client calls the IMMDeviceEnumerator::EnumAudioEndpoints method to create a collection of endpoint objects. Each endpoint object represents an audio endpoint device in the system. In this call, the client specifies whether the collection should contain all of the rendering devices in the system, all of the capture devices, or both.

A device collection is an object with an IMMDeviceCollection interface. Each item in a device collection is an endpoint object with at least the following two interfaces:

An IMMDevice interface. A client obtains a reference to the IMMDevice interface of an endpoint object in a device collection by calling the IMMDeviceCollection::Item method.
An IMMEndpoint interface. A client obtains a reference to the IMMEndpoint interface of an endpoint object by calling the IMMDevice::QueryInterface method.
After retrieving a collection of endpoint devices, the client can query the properties of the individual devices in the collection to determine their suitability for use. For a code example that shows how to enumerate endpoint devices and query their properties, see Device Properties.

After selecting a suitable device, the client can call the IMMDevice::Activate method to activate the device-specific interfaces in WASAPI, the DeviceTopology API, and the EndpointVolume API.

In Windows Vista, the system generates endpoint ID strings to identify the audio endpoint devices in the system. An endpoint ID string is a null-terminated wide-character string. The endpoint ID string for a particular audio endpoint device uniquely identifies the device among all audio endpoint devices in the system.

If a system contains two or more identical audio adapter devices, the corresponding audio endpoint devices will have identical friendly names, but each endpoint device will have a unique endpoint ID string. For more information about obtaining the friendly name of an endpoint device, see Device Properties.

After obtaining an IMMDevice interface instance for an audio endpoint device, a client can call the IMMDevice::GetId method to obtain the endpoint ID string for the device. A client can use the endpoint ID string to create an instance of the audio endpoint device at a later time or in a different process by calling the IMMDeviceEnumerator::GetDevice method.

A client can arrange to receive a notification when the status of any audio endpoint device changes. To receive notifications, the client implements an IMMNotificationClient interface and registers that interface with the MMDevice API. When the status of an endpoint device changes, the MMDevice API calls the appropriate method in the client's EDataFlow interface. One of the input parameters to the method is the endpoint ID string that identifies the endpoint device whose status has changed. For more information about EDataFlow, see Device Events.

Legacy audio APIs such as DirectSound and the Windows multimedia functions have their own interfaces for enumerating and identifying audio devices. In Windows Vista, these interfaces have been extended to supply the endpoint ID strings that identify the endpoint devices that underlie the device abstractions presented by the APIs.

During DirectSound device enumeration, DirectSound supplies the endpoint ID string for each device that it enumerates. For more information, see Audio Events for Legacy Audio Applications.

To obtain the endpoint ID string for a legacy waveform device, use the waveOutMessage or waveInMessage function to send a DRV_QUERYFUNCTIONINSTANCEID message to the waveform device driver. For a code example that shows the use of this message, see Device Roles for Legacy Windows Multimedia Applications.

For more information about using the capabilities of the core audio APIs to enhance applications that use legacy audio APIs, see Interoperability with Legacy Audio APIs.

Clients should treat the contents of the endpoint ID string as opaque. That is, clients should not attempt to parse the contents of the string to obtain information about the device. The reason is that the string format is undefined and might change from one implementation of the MMDevice API system module to the next.

The lifetime of an endpoint ID string is tied to the device installation. The endpoint ID string of a device changes if the user upgrades the device driver, or if the user uninstalls the device, and installs it again. However, the endpoint ID string remains unchanged across system restarts, and the endpoint ID string of a USB audio device remains unchanged if the user unplugs the device and plugs it back in.

Device Properties (Core Audio APIs)
04/18/2023
During the process of enumerating audio endpoint devices, a client application can interrogate the endpoint objects for their device properties. The device properties are exposed in MMDevice API's implementation of the IPropertyStore interface. Given a reference to the IMMDevice interface of an endpoint object, a client can obtain a reference to the endpoint object's property store by calling the IMMDevice::OpenPropertyStore method.

Clients can read these properties, but should not set them. Property values are stored as PROPVARIANT structures.

The endpoint manager sets the basic device properties for endpoints. The endpoint manager is the Windows component that is responsible for detecting the presence of audio endpoint devices.

Each PKEY_Xxx property identifier in the following list is a constant of type PROPERTYKEY that is defined in header file Functiondiscoverykeys_devpkey.h. All audio endpoint devices have these device properties.

Property	Description
PKEY_DeviceInterface_FriendlyName	The friendly name of the audio adapter to which the endpoint device is attached (for example, "XYZ Audio Adapter").
PKEY_Device_DeviceDesc	The device description of the endpoint device (for example, "Speakers").
PKEY_Device_FriendlyName	The friendly name of the endpoint device (for example, "Speakers (XYZ Audio Adapter)").
PKEY_Device_InstanceId	Stores the audio endpoint device instance identifier. The value can also be aquired via IMMDevice::GetId method. For more information about this property, see Endpoint ID Strings and DEVPKEY_Device_InstanceId.
PKEY_Device_ContainerId	Stores the container identifier of the PnP device that implements the audio endpoint. For more information about this property, see DEVPKEY_Device_ContainerId.
Some audio endpoint devices might have additional properties that do not appear in the preceding list. For more information about additional properties, see Audio Endpoint Properties.

For more information about PROPERTYKEY, see the Windows Property System documentation.

The following code example prints the display names of all audio-rendering endpoint devices in the system:

C++

Copy
//-----------------------------------------------------------
// This function enumerates all active (plugged in) audio
// rendering endpoint devices. It prints the friendly name
// and endpoint ID string of each endpoint device.
//-----------------------------------------------------------
#define EXIT_ON_ERROR(hres)  \
              if (FAILED(hres)) { goto Exit; }
#define SAFE_RELEASE(punk)  \
              if ((punk) != NULL)  \
                { (punk)->Release(); (punk) = NULL; }

const CLSID CLSID_MMDeviceEnumerator = __uuidof(MMDeviceEnumerator);
const IID IID_IMMDeviceEnumerator = __uuidof(IMMDeviceEnumerator);

void PrintEndpointNames()
{
    HRESULT hr = S_OK;
    IMMDeviceEnumerator *pEnumerator = NULL;
    IMMDeviceCollection *pCollection = NULL;
    IMMDevice *pEndpoint = NULL;
    IPropertyStore *pProps = NULL;
    LPWSTR pwszID = NULL;

    hr = CoCreateInstance(
           CLSID_MMDeviceEnumerator, NULL,
           CLSCTX_ALL, IID_IMMDeviceEnumerator,
           (void**)&pEnumerator);
    EXIT_ON_ERROR(hr)

    hr = pEnumerator->EnumAudioEndpoints(
                        eRender, DEVICE_STATE_ACTIVE,
                        &pCollection);
    EXIT_ON_ERROR(hr)

    UINT  count;
    hr = pCollection->GetCount(&count);
    EXIT_ON_ERROR(hr)

    if (count == 0)
    {
        printf("No endpoints found.\n");
    }

    // Each loop prints the name of an endpoint device.
    for (ULONG i = 0; i < count; i++)
    {
        // Get pointer to endpoint number i.
        hr = pCollection->Item(i, &pEndpoint);
        EXIT_ON_ERROR(hr)

        // Get the endpoint ID string.
        hr = pEndpoint->GetId(&pwszID);
        EXIT_ON_ERROR(hr)
        
        hr = pEndpoint->OpenPropertyStore(
                          STGM_READ, &pProps);
        EXIT_ON_ERROR(hr)

        PROPVARIANT varName;
        // Initialize container for property value.
        PropVariantInit(&varName);

        // Get the endpoint's friendly-name property.
        hr = pProps->GetValue(
                       PKEY_Device_FriendlyName, &varName);
        EXIT_ON_ERROR(hr)

        // GetValue succeeds and returns S_OK if PKEY_Device_FriendlyName is not found.
        // In this case vartName.vt is set to VT_EMPTY.      
        if (varName.vt != VT_EMPTY)
        {
            // Print endpoint friendly name and endpoint ID.
            printf("Endpoint %d: \"%S\" (%S)\n", 
                    i, varName.pwszVal, pwszID);
        }

        CoTaskMemFree(pwszID);
        pwszID = NULL;
        PropVariantClear(&varName);
        SAFE_RELEASE(pProps)
        SAFE_RELEASE(pEndpoint)
    }
    SAFE_RELEASE(pEnumerator)
    SAFE_RELEASE(pCollection)
    return;

Exit:
    printf("Error!\n");
    CoTaskMemFree(pwszID);
    SAFE_RELEASE(pEnumerator)
    SAFE_RELEASE(pCollection)
    SAFE_RELEASE(pEndpoint)
    SAFE_RELEASE(pProps)
}
The FAILED macro in the preceding code example is defined in header file Winerror.h.

In the preceding code example, the for-loop body in the PrintEndpointNames function calls the IMMDevice::GetId method to obtain the endpoint ID string for the audio endpoint device that is represented by the IMMDevice interface instance. The string uniquely identifies the device with respect to all of the other audio endpoint devices in the system. A client can use the endpoint ID string to create an instance of the audio endpoint device at a later time or in a different process by calling the IMMDeviceEnumerator::GetDevice method. Clients should treat the contents of the endpoint ID string as opaque. That is, clients should not attempt to parse the contents of the string to obtain information about the device. The reason is that the string format is undefined and might change from one implementation of the MMDevice API to the next.

The friendly device names and endpoint ID strings that are obtained by the PrintEndpointNames function in the preceding code example are identical to the friendly device names and endpoint ID strings that are provided by DirectSound during device enumeration. For more information, see Audio Events for Legacy Audio Applications.

In the preceding code example, the PrintEndpointNames function calls the CoCreateInstance function to create an enumerator for the audio endpoint devices in the system. Unless the calling program previously called either the CoInitialize or CoInitializeEx function to initialize the COM library, the CoCreateInstance call will fail. For more information about CoCreateInstance, CoInitialize, and CoInitializeEx, see the Windows SDK documentation.

For more information about the IMMDeviceEnumerator, IMMDeviceCollection, and IMMDevice interfaces, see MMDevice API.

Device Events (Core Audio APIs)
09/28/2022
A device event notifies clients of a change in the status of an audio endpoint device in the system. The following are examples of device events:

The user enables or disables an audio endpoint device from Device Manager or from the Windows multimedia control panel, Mmsys.cpl.
The user adds an audio adapter to the system or removes an audio adapter from the system.
The user plugs an audio endpoint device into an audio jack with jack-presence detection, or removes an audio endpoint device from such a jack.
The user changes the device role that is assigned to a device.
The value of a property of a device changes.
The addition or removal of an audio adapter generates device events for all of the audio endpoint devices that connect to the adapter. The first four items in the preceding list are examples of device state changes. For more information about the device states of audio endpoint devices, see DEVICE_STATE_XXX Constants. For more information about jack-presence detection, see Audio Endpoint Devices.

A client can register to be notified when device events occur. In response to these notifications, the client can dynamically change the way that it uses a particular device, or select a different device to use for a particular purpose.

For example, if an application is playing an audio track through a set of USB speakers, and the user disconnects the speakers from the USB connector, the application receives a device-event notification. In response to the event, if the application detects that a set of desktop speakers is connected to the integrated audio adapter on the system motherboard, the application can resume playing the audio track through the desktop speakers. In this example, the transition from USB speakers to desktop speakers occurs automatically, without requiring the user to intervene by explicitly redirecting the application.

To register to receive device notifications, a client calls the IMMDeviceEnumerator::RegisterEndpointNotificationCallback method. When the client no longer requires notifications, it cancels them by calling the IMMDeviceEnumerator::UnregisterEndpointNotificationCallback method. Both methods take an input parameter, named pClient, that is a pointer to an IMMNotificationClient interface instance.

The IMMNotificationClient interface is implemented by a client. The interface contains several methods, each of which serves as a callback routine for a particular type of device event. When a device event occurs in an audio endpoint device, the MMDevice module calls the appropriate method in the IMMNotificationClient interface of every client that is currently registered to receive device-event notifications. These calls pass a description of the event to the clients. For more information, see IMMNotificationClient Interface.

A client that is registered to receive device-event notifications will receive notifications of all types of device events that occur in all of the audio endpoint devices in the system. If a client is interested only in certain event types or in certain devices, then the methods in its IMMNotificationClient implementation should filter the events appropriately.

The Windows SDK provides samples that include several implementations for the IMMNotificationClient Interface. For more information, see SDK Samples That Use the Core Audio APIs.

The following code example shows a possible implementation of the IMMNotificationClient interface:

C++

Copy
//-----------------------------------------------------------
// Example implementation of IMMNotificationClient interface.
// When the status of audio endpoint devices change, the
// MMDevice module calls these methods to notify the client.
//-----------------------------------------------------------

#define SAFE_RELEASE(punk)  \
              if ((punk) != NULL)  \
                { (punk)->Release(); (punk) = NULL; }

class CMMNotificationClient : public IMMNotificationClient
{
    LONG _cRef;
    IMMDeviceEnumerator *_pEnumerator;

    // Private function to print device-friendly name
    HRESULT _PrintDeviceName(LPCWSTR  pwstrId);

public:
    CMMNotificationClient() :
        _cRef(1),
        _pEnumerator(NULL)
    {
    }

    ~CMMNotificationClient()
    {
        SAFE_RELEASE(_pEnumerator)
    }

    // IUnknown methods -- AddRef, Release, and QueryInterface

    ULONG STDMETHODCALLTYPE AddRef()
    {
        return InterlockedIncrement(&_cRef);
    }

    ULONG STDMETHODCALLTYPE Release()
    {
        ULONG ulRef = InterlockedDecrement(&_cRef);
        if (0 == ulRef)
        {
            delete this;
        }
        return ulRef;
    }

    HRESULT STDMETHODCALLTYPE QueryInterface(
                                REFIID riid, VOID **ppvInterface)
    {
        if (IID_IUnknown == riid)
        {
            AddRef();
            *ppvInterface = (IUnknown*)this;
        }
        else if (__uuidof(IMMNotificationClient) == riid)
        {
            AddRef();
            *ppvInterface = (IMMNotificationClient*)this;
        }
        else
        {
            *ppvInterface = NULL;
            return E_NOINTERFACE;
        }
        return S_OK;
    }

    // Callback methods for device-event notifications.

    HRESULT STDMETHODCALLTYPE OnDefaultDeviceChanged(
                                EDataFlow flow, ERole role,
                                LPCWSTR pwstrDeviceId)
    {
        char  *pszFlow = "?????";
        char  *pszRole = "?????";

        _PrintDeviceName(pwstrDeviceId);

        switch (flow)
        {
        case eRender:
            pszFlow = "eRender";
            break;
        case eCapture:
            pszFlow = "eCapture";
            break;
        }

        switch (role)
        {
        case eConsole:
            pszRole = "eConsole";
            break;
        case eMultimedia:
            pszRole = "eMultimedia";
            break;
        case eCommunications:
            pszRole = "eCommunications";
            break;
        }

        printf("  -->New default device: flow = %s, role = %s\n",
               pszFlow, pszRole);
        return S_OK;
    }

    HRESULT STDMETHODCALLTYPE OnDeviceAdded(LPCWSTR pwstrDeviceId)
    {
        _PrintDeviceName(pwstrDeviceId);

        printf("  -->Added device\n");
        return S_OK;
    };

    HRESULT STDMETHODCALLTYPE OnDeviceRemoved(LPCWSTR pwstrDeviceId)
    {
        _PrintDeviceName(pwstrDeviceId);

        printf("  -->Removed device\n");
        return S_OK;
    }

    HRESULT STDMETHODCALLTYPE OnDeviceStateChanged(
                                LPCWSTR pwstrDeviceId,
                                DWORD dwNewState)
    {
        char  *pszState = "?????";

        _PrintDeviceName(pwstrDeviceId);

        switch (dwNewState)
        {
        case DEVICE_STATE_ACTIVE:
            pszState = "ACTIVE";
            break;
        case DEVICE_STATE_DISABLED:
            pszState = "DISABLED";
            break;
        case DEVICE_STATE_NOTPRESENT:
            pszState = "NOTPRESENT";
            break;
        case DEVICE_STATE_UNPLUGGED:
            pszState = "UNPLUGGED";
            break;
        }

        printf("  -->New device state is DEVICE_STATE_%s (0x%8.8x)\n",
               pszState, dwNewState);

        return S_OK;
    }

    HRESULT STDMETHODCALLTYPE OnPropertyValueChanged(
                                LPCWSTR pwstrDeviceId,
                                const PROPERTYKEY key)
    {
        _PrintDeviceName(pwstrDeviceId);

        printf("  -->Changed device property "
               "{%8.8x-%4.4x-%4.4x-%2.2x%2.2x-%2.2x%2.2x%2.2x%2.2x%2.2x%2.2x}#%d\n",
               key.fmtid.Data1, key.fmtid.Data2, key.fmtid.Data3,
               key.fmtid.Data4[0], key.fmtid.Data4[1],
               key.fmtid.Data4[2], key.fmtid.Data4[3],
               key.fmtid.Data4[4], key.fmtid.Data4[5],
               key.fmtid.Data4[6], key.fmtid.Data4[7],
               key.pid);
        return S_OK;
    }
};

// Given an endpoint ID string, print the friendly device name.
HRESULT CMMNotificationClient::_PrintDeviceName(LPCWSTR pwstrId)
{
    HRESULT hr = S_OK;
    IMMDevice *pDevice = NULL;
    IPropertyStore *pProps = NULL;
    PROPVARIANT varString;

    CoInitialize(NULL);
    PropVariantInit(&varString);

    if (_pEnumerator == NULL)
    {
        // Get enumerator for audio endpoint devices.
        hr = CoCreateInstance(__uuidof(MMDeviceEnumerator),
                              NULL, CLSCTX_INPROC_SERVER,
                              __uuidof(IMMDeviceEnumerator),
                              (void**)&_pEnumerator);
    }
    if (hr == S_OK)
    {
        hr = _pEnumerator->GetDevice(pwstrId, &pDevice);
    }
    if (hr == S_OK)
    {
        hr = pDevice->OpenPropertyStore(STGM_READ, &pProps);
    }
    if (hr == S_OK)
    {
        // Get the endpoint device's friendly-name property.
        hr = pProps->GetValue(PKEY_Device_FriendlyName, &varString);
    }
    printf("----------------------\nDevice name: \"%S\"\n"
           "  Endpoint ID string: \"%S\"\n",
           (hr == S_OK) ? varString.pwszVal : L"null device",
           (pwstrId != NULL) ? pwstrId : L"null ID");

    PropVariantClear(&varString);

    SAFE_RELEASE(pProps)
    SAFE_RELEASE(pDevice)
    CoUninitialize();
    return hr;
}
The CMMNotificationClient class in the preceding code example is an implementation of the IMMNotificationClient interface. Because IMMNotificationClient inherits from IUnknown, the class definition contains implementations of the IUnknown methods AddRef, Release, and QueryInterface. The remaining public methods in the class definition are specific to the IMMNotificationClient interface. These methods are:

OnDefaultDeviceChanged, which is called when the user changes the device role of an audio endpoint device.
OnDeviceAdded, which is called when the user adds an audio endpoint device to the system.
OnDeviceRemoved, which is called when the user removes an audio endpoint device from the system.
OnDeviceStateChanged, which is called when the device state of an audio endpoint device changes. (For more information about device states, see DEVICE_STATE_ XXX Constants.)
OnPropertyValueChanged, which is called when the value of a property of an audio endpoint device changes.
Each of these methods takes an input parameter, pwstrDeviceId, that is a pointer to an endpoint ID string. The string identifies the audio endpoint device in which the device event occurred.

In the preceding code example, _PrintDeviceName is a private method in the CMMNotificationClient class that prints the friendly name of the device. _PrintDeviceName takes the endpoint ID string as an input parameter. It passes the string to the IMMDeviceEnumerator::GetDevice method. GetDevice creates an endpoint device object to represent the device and provides the IMMDevice interface to that object. Next, _PrintDeviceName calls the IMMDevice::OpenPropertyStore method to retrieve the IPropertyStore interface to the device's property store. Finally, _PrintDeviceName calls the IPropertyStore::GetItem method to obtain the friendly-name property of the device. For more information about IPropertyStore, see the Windows SDK documentation.

In addition to device events, clients can register to receive notifications of audio-session events and endpoint-volume events. For more information, see IAudioSessionEvents Interface and IAudioEndpointVolumeCallback Interface.

Device Roles
01/06/2021
If a system contains two or more audio-rendering endpoint devices, then one device might be best for playing one type of audio content, and another device might be best for playing another type of content. For example, if a system has two rendering devices, the user might choose to play music on one device and to play system notification sounds on the other.

Similarly, if a system contains two or more audio-capture endpoint devices, then one device might be best for capturing one type of audio content, and another device might be best for capturing another type of content. For example, if a system has two capture devices, the user might choose to record live music on one device and to use the other device for voice commands.

Devices can have three roles: Console, Communications, and Multimedia.The following table describes the device roles identified by the three constants—eConsole, eCommunications, and eMultimedia—in the ERole enumeration.

ERole constant	Device role	Rendering examples	Capture examples
eConsole	Interaction with the computer	Games and system notifications	Voice commands
eCommunications	Voice communications with another person	Chat and VoIP	Chat and VoIP
eMultimedia	Playing or recording audio content	Music and movies	Narration and live music recording
 

A particular rendering or capture device might be assigned none, one, some, or all of the roles in the preceding table. At any time, each role in the table is assigned to one (and only one) rendering device and to one (and only one) capture device. That is, the assignment of roles to rendering devices is independent of the assignment of roles to capture devices.

An application might choose to play all of its output streams through a single rendering endpoint device, and to record all of its input streams from a single capture endpoint device. Alternatively, an application might choose to play some of its output streams through one rendering device and to play other output streams through another rendering device. Similarly, it might choose to record some of its input streams through one capture device and to record other input streams through another capture device. In all cases, the application can assign each stream to the device whose role is most appropriate for that stream.

For example, a VoIP application might assign the output stream that contains the ring-in notification to the rendering endpoint device with the eConsole role.

Device Formats
01/06/2021
For an audio application, a benefit of using a higher-level audio API, such as DirectSound or the Windows multimedia waveOutXxx functions, is that the API automatically converts between the stream formats used by the application and the formats used by the audio device. In contrast, the core audio APIs are more restrictive because they require application streams to use formats that are the same as, or are closely related to, the formats used by the device. Thus, applications that use the core audio APIs to play or record audio streams might be required to do some or all of the conversions between stream formats.

An application that uses WASAPI to manage shared-mode streams can rely on the audio engine to perform only limited format conversions. The audio engine can convert between a standard PCM sample size used by the application and the floating-point samples that the engine uses for its internal processing. However, the format for an application stream typically must have the same number of channels and the same sample rate as the stream format used by the device.

If an application is using a device in exclusive mode, the application must use a stream format that the audio hardware explicitly supports. In exclusive mode, the application and device exchange audio data directly, without intervention by the audio engine.

Many audio devices support both PCM and non-PCM stream formats. However, the audio engine can mix only PCM streams. Thus, only exclusive-mode streams can have non-PCM formats. In addition, only non-PCM formats with fixed data rates are supported in exclusive mode. An example of a fixed-rate non-PCM format is a 48-kHz Windows Media Audio Professional (WMA Pro) audio stream that passes through a Sony/Philips digital interface (S/PDIF) link in digital form without being decoded. For more information about using WMA Pro streams over S/PDIF, see Specifying WMA Pro Data Ranges.

WASAPI uses a WAVEFORMATEX or WAVEFORMATEXTENSIBLE structure to specify a stream format. A WAVEFORMATEXTENSIBLE structure is effectively a WAVEFORMATEX structure that has been extended to describe a greater range of formats. Any format that can be described by a stand-alone WAVEFORMATEX structure can also be described by a WAVEFORMATEXTENSIBLE structure.

The first member of the WAVEFORMATEXTENSIBLE structure is a WAVEFORMATEX structure. The contents of a WAVEFORMATEX structure indicate whether it is a stand-alone WAVEFORMATEX structure or part of a WAVEFORMATEXTENSIBLE structure.

A stand-alone WAVEFORMATEX structure can adequately describe a format with one or two channels and a sample size that is a multiple of 8 bits. By itself, a WAVEFORMATEX structure cannot specify the mapping of channels to speaker positions. In addition, although WAVEFORMATEX specifies the size of the container for each audio sample, it cannot specify the number of bits of precision in a sample (for example, 20 bits of precision in a 24-bit container). In contrast, the WAVEFORMATEXTENSIBLE structure can specify both the mapping of channels to speakers and the number of bits of precision in each sample.

For more information about WAVEFORMATEX and WAVEFORMATEXTENSIBLE, see the Windows DDK documentation.

Starting with Windows 7, the WAVEFORMATEXTENSIBLE has been extended to represent device formats for transmitting encoded audio over an IEC 61937-compatible interface. For information about the new structure, see Representing Formats for IEC 61937 Transmissions.

Specifying the Device Format
The following WASAPI methods use the WAVEFORMATEX and WAVEFORMATEXTENSIBLE structures to describe stream formats:

IAudioClient::GetMixFormat
IAudioClient::IsFormatSupported
IAudioClient::Initialize
The GetMixFormat method retrieves the stream format that the audio engine uses for its internal processing of shared-mode streams. The method always uses a WAVEFORMATEXTENSIBLE structure, instead of a stand-alone WAVEFORMATEX structure, to specify the format.

The IsFormatSupported method indicates whether an audio endpoint device supports a particular stream format. The caller must specify whether the stream format is intended for use in shared mode or in exclusive mode. For shared-mode formats, the method queries the audio engine to determine whether it supports the specified format. For exclusive-mode formats, the method queries the device driver. Some device drivers will report that they support a 1-channel or 2-channel PCM format if the format is specified by a stand-alone WAVEFORMATEX structure, but will reject the same format if it is specified by a WAVEFORMATEXTENSIBLE structure. To obtain reliable results from these drivers, exclusive-mode applications should call IsFormatSupported twice for each 1-channel or 2-channel PCM format—one call should use a stand-alone WAVEFORMATEX structure to specify the format, and the other call should use a WAVEFORMATEXTENSIBLE structure to specify the same format.

After an application has used GetMixFormat or IsFormatSupported to find an appropriate format for a shared-mode or exclusive-mode stream, the application can call the Initialize method to initialize a stream with that format. An application that attempts to initialize a shared-mode stream with a format that is not identical to the mix format obtained from the GetMixFormat method, but that has the same number of channels and the same sample rate as the mix format, is likely to succeed. Before calling Initialize, the application can call IsFormatSupported to verify that Initialize will accept the format.

The mix format that the audio engine uses for its internal processing of shared-mode streams is closely related to, but is not necessarily identical to, the stream format that the audio endpoint device uses in shared mode. Through the Windows multimedia control panel, Mmsys.cpl, the user can select the stream format that an audio endpoint device will use when it operates in shared mode. The steps are as follows:

To run Mmsys.cpl, open a Command Prompt window and enter the following command:

control mmsys.cpl

Alternatively, you can run Mmsys.cpl by right-clicking the speaker icon in the notification area, which is located on the right side of the taskbar, and selecting either Playback Devices or Recording Devices.

After the Mmsys.cpl window opens, select a device from either the list of playback devices or the list of recording devices, and click Properties.

When the properties window opens, click Advanced, and select a format from the list of available formats in the box labeled Default Format.

For example, assume that the user selects the following default format from the list of available formats for a playback device:

2 channel, 16 bit, 44100 Hz (CD Quality)

This is the format that the device will subsequently use when it operates in shared mode. In Windows Vista, the audio engine will use a slightly modified version of this format for its internal processing of shared-mode streams. The audio engine will use a format with the same number of channels (two) and the same sample rate (44100 Hz), but it will convert samples to floating-point numbers before processing them. The audio engine will convert the floating-point samples in the output mix to 16-bit integers before playing them through the device.

An application can query an audio endpoint device's PKEY_AudioEngine_DeviceFormat property to obtain the shared-mode format that the user has selected for the device. For information about querying the properties of a device, see Device Properties.

Some applications might find the format specified by a device's PKEY_AudioEngine_DeviceFormat property to be a suitable format for opening an exclusive-mode stream on the device. Other applications that manage exclusive-mode streams might have additional requirements that mandate a complex format negotiation with the device. Typically, one of these applications constructs a list of suitable formats, with the preferred formats at the beginning of the list. The application then iteratively calls IsFormatSupported with each successive format in the list, beginning at the start of the list, until it finds a format that the device supports.

Header file Mmdeviceapi.h defines several properties of audio endpoint devices in Windows Vista and later. The Windows audio service sets the values of these properties. Clients can read these properties, but should not set them. Property values are stored as PROPVARIANT structures.

The recommended way of reading the properties of an audio input device is to use the APIs in the Windows.Devices.Enumeration namespace. These APIs are supported for Windows Store apps and desktop apps. For existing desktop apps that read device properties using the IMMDevice interface, see Device Properties. IMMDevice is not supported for Windows Store apps.

For code examples that show how to access the properties of an audio endpoint device, see the following topics:

Device Events
Device Roles for DirectSound Applications
For information about PROPVARIANT, see the Windows SDK documentation.

The following properties are specific to audio endpoint devices.

Property	Description
PKEY_AudioEndpoint_Association	Associates a kernel-streaming (KS) pin category with an audio endpoint device.
PKEY_AudioEndpoint_ControlPanelPageProvider	Specifies the CLSID of the registered provider of the device-properties extension for the audio endpoint device.
PKEY_AudioEndpoint_Disable_SysFx	Indicates whether system effects are enabled in the shared-mode stream that flows to or from the audio endpoint device.
PKEY_AudioEndpoint_FormFactor	Indicates the physical attributes of the audio endpoint device.
PKEY_AudioEndpoint_FullRangeSpeakers	Specifies the channel-configuration mask for the full-range speakers that are connected to the audio endpoint device.
PKEY_AudioEndpoint_GUID	Supplies the DirectSound device identifier that corresponds to the audio endpoint device.
PKEY_AudioEndpoint_PhysicalSpeakers	Defines the physical speaker configuration for the audio endpoint device.
PKEY_AudioEngine_DeviceFormat	Specifies the device format, which is the format that the audio engine uses for the shared-mode stream that flows to or from the audio endpoint device.
PKEY_AudioEngine_OEMFormat	Specifies the default format of the device that is used for rendering or capturing a stream. The values are populated by the OEM in an .inf file.
PKEY_AudioEndpoint_Supports_EventDriven_Mode	Indicates whether the endpoint supports the event-driven mode. The values are populated by the OEM in an .inf file.
PKEY_AudioEndpoint_JackSubType	Contains an output category GUID for an audio endpoint device.
The core audio APIs support additional properties that do not apply exclusively to audio endpoint devices. For more information about these additional properties, see Device Properties.

Clients that manage shared-mode streams typically use the ISimpleAudioVolume and IAudioSessionEvents interfaces in WASAPI to control and monitor the stream volume levels. Through the methods in the ISimpleAudioVolume interface, the client can get and set the volume levels of the audio sessions that the shared-mode streams belong to. If Sndvol or another application changes the session volume level, the client can receive notification of the change through the IAudioSessionEvents interface.

Clients that manage exclusive-mode streams typically use the IAudioEndpointVolume and IAudioEndpointVolumeCallback interfaces in the EndpointVolume API to control and monitor the stream volume levels. Through the methods in the IAudioEndpointVolume interface, the client can get and set the volume level of an audio endpoint device. If Sndvol or another application changes the volume level of the endpoint device, the client can receive notification of the change through the IAudioEndpointVolumeCallback interface.

As explained in Audio Sessions, Sndvol is the system volume-control program. It displays volume controls for the audio-rendering endpoint devices in the system. (Currently, it does not display the volume controls for audio-capture endpoint devices.) To view the volume controls for a particular device, click Device in the menu bar and select a device name from the list of available devices.

The Sndvol window separates the volume controls for a device into two groups. The group box on the left side of the window is labeled Device. The Device box contains a single volume control that is controlled by the IAudioEndpointVolume interface. Changes that the user makes to this volume control can be monitored through the IAudioEndpointVolumeCallback interface.

The group box on the right side of the Sndvol window is labeled Applications. The Applications box contains the volume controls for the applications that currently share the device. For applications that use the device in shared mode, the volume controls represent the volume levels that are controlled by the ISimpleAudioVolume interface. Changes that the user makes to these volume controls can be monitored through the IAudioSessionEvents interface.

Although a shared-mode application can use its IAudioSessionEvents interface to monitor changes that the user makes to the application's volume control in the Applications box in the Sndvol window, the application cannot monitor changes to the volume controls of other, unrelated applications. Similarly, an application can change the volume levels of its audio sessions through the ISimpleAudioVolume interface, but it cannot change the volume levels of sessions that belong to other, unrelated applications.

However, two or more related applications (or instances of the same application) can share the same volume control in the Applications box in the Sndvol window either by assigning their audio streams to the same cross-process session or by associating their respective sessions with the same grouping parameter. For more information, see Audio Sessions and Grouping Parameters.

WASAPI provides two additional interfaces, IChannelAudioVolume and IAudioStreamVolume, to control the volume levels of shared-mode streams. These interfaces are mainly used by specialized clients that require control over the volume levels of either individual channels in a session or individual streams in a session.

The DeviceTopology API enables clients to access the volume controls in the topologies of audio adapters. However, clients that manage exclusive-mode streams typically use the EndpointVolume API instead of the DeviceTopology API to control the stream volume levels. The EndpointVolume API simplifies control of the volume of an endpoint device in two ways. First, if an endpoint device implements a hardware volume control, the DeviceTopology API requires the client to traverse the device topology in search of the hardware control. In contrast, the EndpointVolume API automatically finds the hardware volume control for the client. Second, if the endpoint device does not implement a hardware volume control, a DeviceTopology client must implement a volume control in software. In contrast, the EndpointVolume API automatically substitutes a software volume control for the missing hardware control.

The following sections describe volume controls for audio sessions and for audio endpoint devices:

As explained previously, WASAPI clients can individually control the volume level of each audio session. WASAPI applies the volume setting for a session uniformly to all of the streams in the session. Each volume level is a value in the range 0.0 to 1.0, where 0.0 indicates silence and 1.0 indicates full volume (no attenuation).

A client implicitly creates a session by assigning the first stream to that session. The default volume level of the new session is 1.0. As discussed previously, the user can adjust the volume level of the session through the user interface of a control program (for example, Sndvol) that is a WASAPI client. The control settings are persistent.

In addition to the client-controlled volume settings, the system applies its own volume settings to sessions. These settings are based on audio policy and change dynamically in response to changes in the streams that make up the global audio mix. For more information about audio policy, see User-Mode Audio Components.

The system software that implements the volume control for each stream multiplies the PCM samples in the stream by the effective volume level. The effective volume level is the result of multiplying the client and system volume settings. Thus, the resulting change in signal amplitude is a linear combination of the client and system volume levels. For example, if the client volume level is 0.8 and the system volume level is 0.5, the effective volume level is (0.8).(0.5) = 0.4.

Note that perceived loudness is not linear with respect to signal amplitude. Instead, loudness varies approximately as the logarithm of the volume level v:

loudness in decibels = 20.log₁₀(v)

Thus, setting v = 0.5 attenuates the loudness of the original signal (the signal before the volume level is applied) by 6 decibels, setting v = 0.25 attenuates the signal by 12 decibels, and so on. A volume level v = 1.0, corresponding to 0 decibels, does not alter the original signal level.

Audio applications with user interfaces for controlling the volume level typically display sliders that generate changes in perceived loudness that are linearly proportional to changes in slider position. To produce a linear relationship between perceived loudness and slider position, the application must define a nonlinear relationship between the volume level v and the slider position. For more information, see Audio-Tapered Volume Controls.

As explained previously, the system volume-control program, Sndvol, displays volume sliders for the audio sessions that are playing on each audio rendering device. These sliders appear in the group box labeled Applications in the SndVol window. Typically, each session contains all of the playback streams from a particular application window. Through the sliders in the Sndvol window, users control the volume levels of individual audio applications.

As a general rule, an application should assign all of its playback streams to the same audio session. WASAPI does not prevent an application from distributing its playback streams among multiple sessions. However, the resulting proliferation of volume sliders in Sndvol might confuse users.

As an option, an application window can display a volume slider. The application slider should reflect the state of the corresponding Sndvol slider at all times. Thus, if the user changes the volume level by moving the slider in the application window, then the corresponding slider in the Sndvol window should move in unison with the application slider. Similarly, if the user moves the Sndvol slider, then the application slider should move in unison with the Sndvol slider.

To support this behavior, WASAPI implements the ISimpleAudioVolume interface. When the user moves the application slider, the application calls the ISimpleAudioVolume::SetMasterVolume method to adjust the session volume level accordingly. Sndvol monitors volume changes made through this method and reflects the changes in the volume sliders that it displays. In addition, an application can receive notifications of session volume changes that the user makes through Sndvol. For this purpose, the application implements an IAudioSessionEvents interface and registers the interface with WASAPI. Thereafter, each time the user changes the session volume level through Sndvol, the application receives a notification call through the IAudioSessionEvents::OnSimpleVolumeChanged method. For a code example that implements an IAudioSessionEvents interface, see Audio Session Events. For a code example that registers an IAudioSessionEvents interface, see Audio Events for Legacy Audio Applications.

The ISimpleAudioVolume interface applies the same volume level uniformly to all of the channels in an audio session. Although this interface should satisfy the volume-control requirements of most applications, a few applications might require more specialized volume-control capabilities. The IAudioStreamVolume interface controls the volume of an individual stream in a session relative to the other streams in the session. IAudioStreamVolume also enables a client to individually control the volume levels of all the channels in the stream. For example, an application might use this capability to achieve audio effects such as simulating spatial movement of an audio source by panning from left to right. Another specialized interface, IChannelAudioVolume, controls the volume levels of the individual channels in a session. For example, an application might use IChannelAudioVolume to implement balance controls for a stereophonic sound system.

The volume sliders in the Applications box in Sndvol reflect only volume changes that are made through the ISimpleAudioVolume interface. They do not reflect volume changes that are made through the IAudioStreamVolume and IChannelAudioVolume interfaces. Although some applications might enable users to directly or indirectly control volume settings through IAudioStreamVolume and IChannelAudioVolume, developers should avoid presenting application sliders for these volume settings that users are likely to confuse with the volume sliders in Sndvol. Otherwise, a user might move an application slider expecting to see the change reflected in a Sndvol slider and become confused when no such change occurs. Developers can avoid this problem through careful user interface design.

The effective volume level of any channel in the session submix, as heard at the speakers, is the product of the following four volume-level factors:

The per-channel volume levels of the streams in the session, which clients can control through the methods in the IAudioStreamVolume interface.
The per-channel volume level of the session, which clients can control through the methods in the IChannelAudioVolume interface.
The master volume level of the session, which clients can control through the methods in the ISimpleAudioVolume interface.
The policy-based volume level of the session, which the system dynamically modifies as the global mix changes.
Each of the four volume-level factors in the preceding list is a value in the range 0.0 to 1.0, where 0.0 indicates silence and 1.0 indicates full volume (no attenuation). The effective volume level is also a value in the range 0.0 to 1.0.

The audio engine applies the effective volume level for each channel to the channels in a stream before mixing the stream with the other streams in the audio session. If any sample values in a channel exceed 0 decibels after the audio engine has multiplied them by the effective volume level, the engine clips the samples before adding them to the session submix.

Related topics

The ISimpleAudioVolume, IChannelAudioVolume, and IAudioStreamVolume interfaces enable clients to control the volume levels of audio sessions, which are collections of shared-mode audio streams. These interfaces do not work with exclusive-mode audio streams.

Applications that manage exclusive-mode streams can control the volume levels of those streams through the IAudioEndpointVolume interface. This interface controls the volume level of the audio endpoint device. It uses the hardware volume control for the endpoint device if the audio hardware implements such a control. Otherwise, the IAudioEndpointVolume interface implements the volume control in software.

If a device has a hardware volume control, changes made to the control through the IAudioEndpointVolume interface affect the volume level both in shared mode and in exclusive mode. If a device lacks hardware volume and mute controls, changes made to the software volume and mute controls through this interface affect the volume level in shared mode, but not in exclusive mode. In exclusive mode, the application and the audio hardware exchange audio data directly, bypassing the software controls.

As a general rule, applications should avoid using the IAudioEndpointVolume interface to control the volume levels of shared-mode streams. Instead, applications should use the ISimpleAudioVolume, IChannelAudioVolume, or IAudioStreamVolume interface for that purpose.

If an application displays a volume control that uses the IAudioEndpointVolume interface to control the volume level of an audio endpoint device, that volume control should mirror the endpoint volume control displayed by the system volume-control program, Sndvol. As explained previously, the endpoint volume control appears on the left side of the Sndvol window, in the group box labeled Device. If the user changes the endpoint volume of a device through the volume control in Sndvol, the corresponding endpoint volume control in the application should move in unison with the control in Sndvol. Similarly, if the user changes the volume level through the endpoint volume control in the application window, the corresponding volume control in Sndvol should move in unison with the application's volume control.

To ensure that the endpoint volume control in an application window mirrors the endpoint volume control in Sndvol, the application should implement an IAudioEndpointVolumeCallback interface and register that interface to receive notifications. Thereafter, each time the user changes the endpoint volume level in Sndvol, the application receives a notification call through its IAudioEndpointVolumeCallback::OnNotify method. During this call, the OnNotify method can update the endpoint volume control in the application window to match the control setting shown in Sndvol. Similarly, each time the user changes the endpoint volume level through volume control in the application window, Sndvol receives a notification and immediately updates its endpoint volume control to display the new volume level.

The following code example is a header file that shows a possible implementation of the IAudioEndpointVolumeCallback interface:

C++

Copy
// Epvolume.h -- Implementation of IAudioEndpointVolumeCallback interface

#include <windows.h>
#include <commctrl.h>
#include <mmdeviceapi.h>
#include <endpointvolume.h>
#include "resource.h"

// Dialog handle from dialog box procedure
extern HWND g_hDlg;

// Client's proprietary event-context GUID
extern GUID g_guidMyContext;

// Maximum volume level on trackbar
#define MAX_VOL  100

#define SAFE_RELEASE(punk)  \
              if ((punk) != NULL)  \
                { (punk)->Release(); (punk) = NULL; }

//-----------------------------------------------------------
// Client implementation of IAudioEndpointVolumeCallback
// interface. When a method in the IAudioEndpointVolume
// interface changes the volume level or muting state of the
// endpoint device, the change initiates a call to the
// client's IAudioEndpointVolumeCallback::OnNotify method.
//-----------------------------------------------------------
class CAudioEndpointVolumeCallback : public IAudioEndpointVolumeCallback
{
    LONG _cRef;

public:
    CAudioEndpointVolumeCallback() :
        _cRef(1)
    {
    }

    ~CAudioEndpointVolumeCallback()
    {
    }

    // IUnknown methods -- AddRef, Release, and QueryInterface

    ULONG STDMETHODCALLTYPE AddRef()
    {
        return InterlockedIncrement(&_cRef);
    }

    ULONG STDMETHODCALLTYPE Release()
    {
        ULONG ulRef = InterlockedDecrement(&_cRef);
        if (0 == ulRef)
        {
            delete this;
        }
        return ulRef;
    }

    HRESULT STDMETHODCALLTYPE QueryInterface(REFIID riid, VOID **ppvInterface)
    {
        if (IID_IUnknown == riid)
        {
            AddRef();
            *ppvInterface = (IUnknown*)this;
        }
        else if (__uuidof(IAudioEndpointVolumeCallback) == riid)
        {
            AddRef();
            *ppvInterface = (IAudioEndpointVolumeCallback*)this;
        }
        else
        {
            *ppvInterface = NULL;
            return E_NOINTERFACE;
        }
        return S_OK;
    }

    // Callback method for endpoint-volume-change notifications.

    HRESULT STDMETHODCALLTYPE OnNotify(PAUDIO_VOLUME_NOTIFICATION_DATA pNotify)
    {
        if (pNotify == NULL)
        {
            return E_INVALIDARG;
        }
        if (g_hDlg != NULL && pNotify->guidEventContext != g_guidMyContext)
        {
            PostMessage(GetDlgItem(g_hDlg, IDC_CHECK_MUTE), BM_SETCHECK,
                        (pNotify->bMuted) ? BST_CHECKED : BST_UNCHECKED, 0);

            PostMessage(GetDlgItem(g_hDlg, IDC_SLIDER_VOLUME),
                        TBM_SETPOS, TRUE,
                        LPARAM((UINT32)(MAX_VOL*pNotify->fMasterVolume + 0.5)));
        }
        return S_OK;
    }
};
The CAudioEndpointVolumeCallback class in the preceding code example is an implementation of the IAudioEndpointVolumeCallback interface. Because IAudioEndpointVolumeCallback inherits from IUnknown, the class definition contains implementations of the IUnknown methods AddRef, Release, and QueryInterface. The OnNotify method in the class definition is called each time one of the following methods changes the endpoint volume level:

IAudioEndpointVolume::SetChannelVolumeLevel
IAudioEndpointVolume::SetChannelVolumeLevelScalar
IAudioEndpointVolume::SetMasterVolumeLevel
IAudioEndpointVolume::SetMasterVolumeLevelScalar
IAudioEndpointVolume::SetMute
IAudioEndpointVolume::VolumeStepDown
IAudioEndpointVolume::VolumeStepUp
The implementation of the OnNotify method in the preceding code example sends messages to the volume control in the application window to update the displayed volume level.

An application calls the IAudioEndpointVolume::RegisterControlChangeNotify method to register its IAudioEndpointVolumeCallback interface to receive notifications. When the application no longer requires notifications, it calls the IAudioEndpointVolume::UnregisterControlChangeNotify method to delete the registration.

The following code example is a Windows application that calls the RegisterControlChangeNotify and UnregisterControlChangeNotify methods to register and unregister the CAudioEndpointVolumeCallback class in the preceding code example:

C++

Copy
// Epvolume.cpp -- WinMain and dialog box functions

#include "stdafx.h"
#include "Epvolume.h"

HWND g_hDlg = NULL;
GUID g_guidMyContext = GUID_NULL;

static IAudioEndpointVolume *g_pEndptVol = NULL;
static BOOL CALLBACK DlgProc(HWND, UINT, WPARAM, LPARAM);

#define EXIT_ON_ERROR(hr)  \
              if (FAILED(hr)) { goto Exit; }
#define ERROR_CANCEL(hr)  \
              if (FAILED(hr)) {  \
                  MessageBox(hDlg, TEXT("The program will exit."),  \
                             TEXT("Fatal error"), MB_OK);  \
                  EndDialog(hDlg, TRUE); return TRUE; }

//-----------------------------------------------------------
// WinMain -- Registers an IAudioEndpointVolumeCallback
//   interface to monitor endpoint volume level, and opens
//   a dialog box that displays a volume control that will
//   mirror the endpoint volume control in SndVol.
//-----------------------------------------------------------
int APIENTRY WinMain(HINSTANCE hInstance,
                     HINSTANCE hPrevInstance,
                     LPSTR lpCmdLine,
                     int nCmdShow)
{
    HRESULT hr = S_OK;
    IMMDeviceEnumerator *pEnumerator = NULL;
    IMMDevice *pDevice = NULL;
    CAudioEndpointVolumeCallback EPVolEvents;

    if (hPrevInstance)
    {
        return 0;
    }

    CoInitialize(NULL);

    hr = CoCreateGuid(&g_guidMyContext);
    EXIT_ON_ERROR(hr)

    // Get enumerator for audio endpoint devices.
    hr = CoCreateInstance(__uuidof(MMDeviceEnumerator),
                          NULL, CLSCTX_INPROC_SERVER,
                          __uuidof(IMMDeviceEnumerator),
                          (void**)&pEnumerator);
    EXIT_ON_ERROR(hr)

    // Get default audio-rendering device.
    hr = pEnumerator->GetDefaultAudioEndpoint(eRender, eConsole, &pDevice);
    EXIT_ON_ERROR(hr)

    hr = pDevice->Activate(__uuidof(IAudioEndpointVolume),
                           CLSCTX_ALL, NULL, (void**)&g_pEndptVol);
    EXIT_ON_ERROR(hr)

    hr = g_pEndptVol->RegisterControlChangeNotify(
                     (IAudioEndpointVolumeCallback*)&EPVolEvents);
    EXIT_ON_ERROR(hr)

    InitCommonControls();
    DialogBox(hInstance, L"VOLUMECONTROL", NULL, (DLGPROC)DlgProc);

Exit:
    if (FAILED(hr))
    {
        MessageBox(NULL, TEXT("This program requires Windows Vista."),
                   TEXT("Error termination"), MB_OK);
    }
    if (g_pEndptVol != NULL)
    {
        g_pEndptVol->UnregisterControlChangeNotify(
                    (IAudioEndpointVolumeCallback*)&EPVolEvents);
    }
    SAFE_RELEASE(pEnumerator)
    SAFE_RELEASE(pDevice)
    SAFE_RELEASE(g_pEndptVol)
    CoUninitialize();
    return 0;
}

//-----------------------------------------------------------
// DlgProc -- Dialog box procedure
//-----------------------------------------------------------

BOOL CALLBACK DlgProc(HWND hDlg, UINT message, WPARAM wParam, LPARAM lParam)
{
    HRESULT hr;
    BOOL bMute;
    float fVolume;
    int nVolume;
    int nChecked;

    switch (message)
    {
    case WM_INITDIALOG:
        g_hDlg = hDlg;
        SendDlgItemMessage(hDlg, IDC_SLIDER_VOLUME, TBM_SETRANGEMIN, FALSE, 0);
        SendDlgItemMessage(hDlg, IDC_SLIDER_VOLUME, TBM_SETRANGEMAX, FALSE, MAX_VOL);
        hr = g_pEndptVol->GetMute(&bMute);
        ERROR_CANCEL(hr)
        SendDlgItemMessage(hDlg, IDC_CHECK_MUTE, BM_SETCHECK,
                           bMute ? BST_CHECKED : BST_UNCHECKED, 0);
        hr = g_pEndptVol->GetMasterVolumeLevelScalar(&fVolume);
        ERROR_CANCEL(hr)
        nVolume = (int)(MAX_VOL*fVolume + 0.5);
        SendDlgItemMessage(hDlg, IDC_SLIDER_VOLUME, TBM_SETPOS, TRUE, nVolume);
        return TRUE;

    case WM_HSCROLL:
        switch (LOWORD(wParam))
        {
        case SB_THUMBPOSITION:
        case SB_THUMBTRACK:
        case SB_LINERIGHT:
        case SB_LINELEFT:
        case SB_PAGERIGHT:
        case SB_PAGELEFT:
        case SB_RIGHT:
        case SB_LEFT:
            // The user moved the volume slider in the dialog box.
            SendDlgItemMessage(hDlg, IDC_CHECK_MUTE, BM_SETCHECK, BST_UNCHECKED, 0);
            hr = g_pEndptVol->SetMute(FALSE, &g_guidMyContext);
            ERROR_CANCEL(hr)
            nVolume = SendDlgItemMessage(hDlg, IDC_SLIDER_VOLUME, TBM_GETPOS, 0, 0);
            fVolume = (float)nVolume/MAX_VOL;
            hr = g_pEndptVol->SetMasterVolumeLevelScalar(fVolume, &g_guidMyContext);
            ERROR_CANCEL(hr)
            return TRUE;
        }
        break;

    case WM_COMMAND:
        switch ((int)LOWORD(wParam))
        {
        case IDC_CHECK_MUTE:
            // The user selected the Mute check box in the dialog box.
            nChecked = SendDlgItemMessage(hDlg, IDC_CHECK_MUTE, BM_GETCHECK, 0, 0);
            bMute = (BST_CHECKED == nChecked);
            hr = g_pEndptVol->SetMute(bMute, &g_guidMyContext);
            ERROR_CANCEL(hr)
            return TRUE;
        case IDCANCEL:
            EndDialog(hDlg, TRUE);
            return TRUE;
        }
        break;
    }
    return FALSE;
}
In the preceding code example, the WinMain function calls the CoCreateInstance function to create an instance of the IMMDeviceEnumerator interface, and it calls the IMMDeviceEnumerator::GetDefaultAudioEndpoint method to obtain the IMMDevice interface of the default rendering device. WinMain calls the IMMDevice::Activate method to obtain the device's IAudioEndpointVolume interface, and it calls RegisterControlChangeNotify to register the application to receive notifications of endpoint volume changes. Next, WinMain opens a dialog box to display an endpoint volume control for the device. The dialog box also displays a check box that indicates whether the device is muted. The endpoint volume control and mute check box in the dialog box mirror the settings of the endpoint volume control and mute check box displayed by Sndvol. For more information about WinMain and CoCreateInstance, see the Windows SDK documentation. For more information about IMMDeviceEnumerator and IMMDevice, see Enumerating Audio Devices.

The dialog box procedure, DlgProc, in the preceding code example, handles the changes that the user makes to the volume and mute settings through the controls in the dialog box. When DlgProc calls SetMasterVolumeLevelScalar or SetMute, Sndvol receives notification of the change and updates the corresponding control in its window to reflect the new volume or mute setting. If, instead of using the dialog box, the user updates the volume and mute settings through the controls in the Sndvol window, the OnNotify method in the CAudioEndpointVolumeCallback class updates the controls in the dialog box to display the new settings.

If the user changes the volume through the controls in the dialog box, the OnNotify method in the CAudioEndpointVolumeCallback class does not send messages to update the controls in the dialog box. To do so would be redundant. OnNotify updates the controls in the dialog box only if the volume change originated in Sndvol or in some other application. The second parameter in the SetMasterVolumeLevelScalar and SetMute method calls in the DlgProc function is a pointer to an event-context GUID that either method passes to OnNotify. OnNotify checks the value of the event-context GUID to determine whether the dialog box is the source of the volume change. For more information about event-context GUIDs, see IAudioEndpointVolumeCallback::OnNotify.

When the user exits the dialog box, the UnregisterControlChangeNotify call in the preceding code example deletes the registration of the CAudioEndpointVolumeCallback class before the program terminates.

You can easily modify the preceding code example to display volume and mute controls for the default capture device. In the WinMain function, change the value of the first parameter in the call to the IMMDeviceEnumerator::GetDefaultAudioEndpoint method from eRender to eCapture.

The following code example is the resource script that defines the volume and mute controls that appear in the preceding code example:

C++

Copy
// Epvolume.rc -- Resource script

#include "resource.h"
#include "windows.h"
#include "commctrl.h"

//
// Dialog box
//
VOLUMECONTROL DIALOGEX 0, 0, 160, 60
STYLE DS_MODALFRAME | WS_CAPTION | WS_SYSMENU | DS_SETFONT
CAPTION "Audio Endpoint Volume"
FONT 8, "Arial Rounded MT Bold", 400, 0, 0x0
BEGIN
    LTEXT      "Min",IDC_STATIC_MINVOL,10,10,20,12
    RTEXT      "Max",IDC_STATIC_MAXVOL,130,10,20,12
    CONTROL    "",IDC_SLIDER_VOLUME,"msctls_trackbar32",
               TBS_BOTH | TBS_NOTICKS | WS_TABSTOP,10,20,140,12
    CONTROL    "Mute",IDC_CHECK_MUTE,"Button",
               BS_AUTOCHECKBOX | WS_TABSTOP,20,40,70,12
END
The following code example is the resource header file that defines the control identifiers that appear in the preceding code examples:

C++

Copy
// Resource.h -- Control identifiers (epvolume)

#define IDC_SLIDER_VOLUME      1001
#define IDC_CHECK_MUTE         1002
#define IDC_STATIC_MINVOL      1003
#define IDC_STATIC_MAXVOL      1004
The preceding code examples combine to form a simple application for controlling and monitoring the endpoint volume of the default rendering device. A more useful application might additionally notify the user when the status of the device changes. For example, the device might be disabled, unplugged, or removed. For more information about monitoring these types of events, see Device Events.

EndpointVolume API
01/06/2021
The EndpointVolume API enables specialized clients to control and monitor the volume levels of audio endpoint devices. A client obtains references to the interfaces in the EndpointVolume API by obtaining the IMMDevice interface of an audio endpoint device and calling the IMMDevice::Activate method.

Header file Endpointvolume.h defines the interfaces in the EndpointVolume API.

Audio applications that use the MMDevice API and WASAPI typically use the ISimpleAudioVolume interface to control volume levels on a per-session basis. Only two types of audio applications require the use of the EndpointVolume API. These application types are:

Applications that manage the master volume levels of audio endpoint devices, similar to the Windows volume-control program, Sndvol.exe.
Professional audio ("pro audio") applications that require exclusive-mode access to audio endpoint devices.
Inappropriate use of the EndpointVolume API can interfere with Windows audio policy and disrupt the user's system volume settings.

If an audio endpoint device implements hardware volume and mute controls, the EndpointVolume API uses those controls to manage the device volume. Otherwise, the EndpointVolume API implements the controls in software, transparently to the client.

If a device has hardware volume and mute controls, changes made to the device's volume and mute settings through the EndpointVolume API affect the volume level in both shared mode and exclusive mode. If a device lacks hardware volume and mute controls, changes made to the software volume and mute controls through the EndpointVolume API affect the volume level in shared mode, but not in exclusive mode. In exclusive mode, the client and the device exchange audio data directly, bypassing the software controls.

For applications that must manage hardware volume and mute controls, the EndpointVolume API offers two potential advantages over the DeviceTopology API.

First, a number of audio adapter devices lack hardware volume controls. If a device lacks a hardware volume control, the IAudioEndpointVolume interface in the EndpointVolume API automatically implements a software volume control on the stream to or from that device. For a client of the EndpointVolume API, the result is the same whether the volume control is implemented in hardware by the device or in software by the EndpointVolume API interface.

Second, even if the adapter device does implement hardware volume controls, an application that uses the DeviceTopology API to implement a topology-traversal algorithm might fail to find the control that it is looking for. Typically, such an application is designed to traverse the hardware topology of a particular device or set of related devices. The application risks failing if it attempts to traverse the topology of a device that it has not been specifically designed for or tested with.

Only specialized applications that must access hardware functions other than volume and mute controls require the use of the DeviceTopology API. For applications that require control only of the volume level of an exclusive-mode stream, the EndpointVolume API is simpler to use and works reliably with a wider range of audio hardware devices.

For code examples that use the interfaces in the EndpointVolume API, see the following topics:

Endpoint Volume Controls
Peak Meters
To see a sample that uses the EndpointVolume API, see EndpointVolume in the Windows SDK.

The EndpointVolume API implements the following interfaces.

Interface	Description
IAudioEndpointVolume	Represents the volume controls on the audio stream to or from an audio endpoint device.
IAudioMeterInformation	Represents a peak meter on the audio stream to or from an audio endpoint device.
 

In addition, clients of the EndpointVolume API that require notification of volume and muting changes in audio endpoint devices should implement the following interface.

Interface	Description
IAudioEndpointVolumeCallback	Provides notifications when the volume level or muting state of an audio endpoint device changes.
The IAudioEndpointVolume interface manages volume controls that are audio tapered. These controls are well suited to Windows applications that display volume sliders. For a volume slider that is tied to an audio-tapered volume control, each change in the position of the slider produces a change in perceived loudness that is proportional to the distance traveled by the slider. For a particular travel distance, the amount by which the perceived loudness increases or decreases is approximately the same regardless of whether the slider movement occurs in the lower, upper, or middle portion of the slider's range of movement. Perceived loudness varies approximately linearly with the logarithm of the audio signal power.

The term audio taper originally referred to the tapered shape of the resistive element in a potentiometer that is used as a volume control in an audio electronics device. An audio-tapered resistive element is widest at the zero-volume position and narrowest at the maximum-volume position. The potentiometer controls the voltage level of the audio signal that the device plays through its speakers. The tapering is designed to produce an approximately linear relationship between the position of the potentiometer wiper and the perceived loudness at the speakers. The relationship between the wiper position and the voltage at the speakers is nonlinear.

In contrast, a resistive element with a linear taper has a uniform width over the potentiometer wiper's range of movement. As a result, the voltage at the speakers varies linearly with the wiper position. The relationship between the wiper position and loudness is nonlinear.

Similarly, a Windows application that displays a volume slider defines a relationship between the slider position and the output signal level at the speakers. The relationship can, in effect, be linear tapered or audio tapered.

The following diagram shows the mapping of slider position to output voltage and to perceived loudness for a linear-tapered volume control.

output diagram for a linear-tapered volume control

On the left side of the preceding diagram, the output voltage level of the audio digital-to-analog converter (DAC) increases linearly as the volume slider moves from its minimum position (labeled Min) to its maximum position (labeled Max). The label VFS on the vertical axis represents the full-scale DAC output voltage.

However, perceived loudness varies approximately as the logarithm of the power of the audio signal, as shown on the right side of the preceding diagram. Thus, movement of the slider over an interval near the minimum setting results in a relatively large change in perceived loudness, but slider movement over an interval of the same width near the maximum setting causes a relatively small change in perceived loudness.

On the right side of the preceding diagram, loudness on the vertical axis is measured in decibels (dB) relative to the full-scale power setting (at 0 decibels). The loudness curve intersects the vertical axis at minus infinity, but only the portion of the curve from 0 decibels to –96 decibels appears in the diagram. The decision to show only this portion of the curve is somewhat arbitrary, but –96 decibels conveniently represents the power at the next-to-lowest output level of a 16-bit DAC relative to the full-scale power. This value is calculated as 20.log₁₀(1/65535).

Because small changes in slider position near the minimum setting in the preceding diagram result in large changes in loudness, the user might find the volume difficult to control over this region. Relatively small slider movements can push the volume well above or below the desired level. An improved volume control would provide a more linear relationship between slider position and loudness.

The following diagram shows the mapping of slider position to output voltage and to perceived loudness for an audio-tapered volume control.

output diagram for audio-tapered volume control

As shown on the right side of the preceding diagram, perceived loudness varies approximately linearly with changes in slider position. For this to occur, the DAC voltage must vary nonlinearly with position, as shown on the left side of the diagram. The curve asymptotically approaches 0 volts as the slider moves toward the left from the maximum setting. The voltage at the minimum slider position is very small, but it might not be exactly zero.

The following methods in the IAudioEndpointVolume interface use volume settings that are measured in decibels:

IAudioEndpointVolume::GetChannelVolumeLevel
IAudioEndpointVolume::GetMasterVolumeLevel
IAudioEndpointVolume::SetChannelVolumeLevel
IAudioEndpointVolume::SetMasterVolumeLevel
These methods produce an approximately linear relationship between volume setting and perceived loudness. The volume range in decibels of the volume controls that are managed by these methods depends on the audio endpoint device. To determine the volume range for a particular device, call the IAudioEndpointVolume::GetVolumeRange method.

In contrast, the volume settings for the following methods in the IAudioEndpointVolume interface follow a more gently tapered curve over the volume range:

IAudioEndpointVolume::GetChannelVolumeLevelScalar
IAudioEndpointVolume::GetMasterVolumeLevelScalar
IAudioEndpointVolume::SetChannelVolumeLevelScalar
IAudioEndpointVolume::SetMasterVolumeLevelScalar
IAudioEndpointVolume::VolumeStepDown
IAudioEndpointVolume::VolumeStepUp
In Windows Vista, these methods use a curve that is intermediate between the audio-tapered curve shown in the preceding diagram and a linear-tapered curve. Note that the shape of the curve might change in future versions of Windows. The first four methods in the preceding list express volume levels as normalized values in the range from 0.0 (minimum volume) to 1.0 (maximum volume). For the last two methods in the list, call the IAudioEndpointVolume::GetVolumeStepInfo method to obtain the number of steps in the volume range.

The following interfaces use linear-tapered curves for their volume settings:

ISimpleAudioVolume
IChannelAudioVolume
IAudioStreamVolume
For more information about these interfaces, see Session Volume Controls. And for information about the volume ranges and the default volume levels in the various versions of Windows, see Default Audio Volume Settings.

To support Windows applications that display peak meters, the EndpointVolume API includes an IAudioMeterInformation interface. This interface represents a peak meter on an audio endpoint device. For a rendering device, the value retrieved from the peak meter represents the maximum sample value encountered in the output stream to the device during the preceding metering period. For a capture device, the value retrieved from the peak meter represents the maximum sample value encountered in the input stream from the device.

The peak-meter values obtained from the methods in the IAudioMeterInformation interface are floating-point numbers in the normalized range from 0.0 to 1.0. For example, if a PCM stream contains 16-bit samples, and the peak sample value during a particular metering period is —8914, then the absolute value recorded by the peak meter is 8914, and the normalized peak value reported by the IAudioMeterInformation interface is 8914/32768 = 0.272.

If the audio endpoint device implements the peak meter in hardware, the IAudioMeterInformation interface uses the hardware peak meter. Otherwise, the interface implements the peak meter in software.

If a device has a hardware peak meter, the peak meter is active both in shared mode and in exclusive mode. If a device lacks hardware peak meter, the peak meter is active in shared mode, but not in exclusive mode. In exclusive mode, the application and the audio hardware exchange audio data directly, bypassing the software peak meter (which always reports a peak value of 0.0).

The following C++ code example is a Windows application that displays a peak meter for the default rendering device:

C++

Copy
// Peakmeter.cpp -- WinMain and dialog box functions

#include <windows.h>
#include <mmdeviceapi.h>
#include <endpointvolume.h>
#include "resource.h"

static BOOL CALLBACK DlgProc(HWND, UINT, WPARAM, LPARAM);
static void DrawPeakMeter(HWND, float);

// Timer ID and period (in milliseconds)
#define ID_TIMER  1
#define TIMER_PERIOD  125

#define EXIT_ON_ERROR(hr)  \
              if (FAILED(hr)) { goto Exit; }
#define SAFE_RELEASE(punk)  \
              if ((punk) != NULL)  \
                { (punk)->Release(); (punk) = NULL; }

//-----------------------------------------------------------
// WinMain -- Opens a dialog box that contains a peak meter.
//   The peak meter displays the peak sample value that plays
//   through the default rendering device.
//-----------------------------------------------------------
int APIENTRY WinMain(HINSTANCE hInstance,
                     HINSTANCE hPrevInstance,
                     LPSTR lpCmdLine,
                     int nCmdShow)
{
    HRESULT hr;
    IMMDeviceEnumerator *pEnumerator = NULL;
    IMMDevice *pDevice = NULL;
    IAudioMeterInformation *pMeterInfo = NULL;

    if (hPrevInstance)
    {
        return 0;
    }

    CoInitialize(NULL);

    // Get enumerator for audio endpoint devices.
    hr = CoCreateInstance(__uuidof(MMDeviceEnumerator),
                          NULL, CLSCTX_INPROC_SERVER,
                          __uuidof(IMMDeviceEnumerator),
                          (void**)&pEnumerator);
    EXIT_ON_ERROR(hr)

    // Get peak meter for default audio-rendering device.
    hr = pEnumerator->GetDefaultAudioEndpoint(eRender, eConsole, &pDevice);
    EXIT_ON_ERROR(hr)

    hr = pDevice->Activate(__uuidof(IAudioMeterInformation),
                           CLSCTX_ALL, NULL, (void**)&pMeterInfo);
    EXIT_ON_ERROR(hr)

    DialogBoxParam(hInstance, L"PEAKMETER", NULL, (DLGPROC)DlgProc, (LPARAM)pMeterInfo);

Exit:
    if (FAILED(hr))
    {
        MessageBox(NULL, TEXT("This program requires Windows Vista."),
                   TEXT("Error termination"), MB_OK);
    }
    SAFE_RELEASE(pEnumerator)
    SAFE_RELEASE(pDevice)
    SAFE_RELEASE(pMeterInfo)
    CoUninitialize();
    return 0;
}

//-----------------------------------------------------------
// DlgProc -- Dialog box procedure
//-----------------------------------------------------------

BOOL CALLBACK DlgProc(HWND hDlg, UINT message, WPARAM wParam, LPARAM lParam)
{
    static IAudioMeterInformation *pMeterInfo = NULL;
    static HWND hPeakMeter = NULL;
    static float peak = 0;
    HRESULT hr;

    switch (message)
    {
    case WM_INITDIALOG:
        pMeterInfo = (IAudioMeterInformation*)lParam;
        SetTimer(hDlg, ID_TIMER, TIMER_PERIOD, NULL);
        hPeakMeter = GetDlgItem(hDlg, IDC_PEAK_METER);
        return TRUE;

    case WM_COMMAND:
        switch ((int)LOWORD(wParam))
        {
        case IDCANCEL:
            KillTimer(hDlg, ID_TIMER);
            EndDialog(hDlg, TRUE);
            return TRUE;
        }
        break;

    case WM_TIMER:
        switch ((int)wParam)
        {
        case ID_TIMER:
            // Update the peak meter in the dialog box.
            hr = pMeterInfo->GetPeakValue(&peak);
            if (FAILED(hr))
            {
                MessageBox(hDlg, TEXT("The program will exit."),
                           TEXT("Fatal error"), MB_OK);
                KillTimer(hDlg, ID_TIMER);
                EndDialog(hDlg, TRUE);
                return TRUE;
            }
            DrawPeakMeter(hPeakMeter, peak);
            return TRUE;
        }
        break;

    case WM_PAINT:
        // Redraw the peak meter in the dialog box.
        ValidateRect(hPeakMeter, NULL);
        DrawPeakMeter(hPeakMeter, peak);
        break;
    }
    return FALSE;
}

//-----------------------------------------------------------
// DrawPeakMeter -- Draws the peak meter in the dialog box.
//-----------------------------------------------------------

void DrawPeakMeter(HWND hPeakMeter, float peak)
{
    HDC hdc;
    RECT rect;

    GetClientRect(hPeakMeter, &rect);
    hdc = GetDC(hPeakMeter);
    FillRect(hdc, &rect, (HBRUSH)(COLOR_3DSHADOW+1));
    rect.left++;
    rect.top++;
    rect.right = rect.left +
                 max(0, (LONG)(peak*(rect.right-rect.left)-1.5));
    rect.bottom--;
    FillRect(hdc, &rect, (HBRUSH)(COLOR_3DHIGHLIGHT+1));
    ReleaseDC(hPeakMeter, hdc);
}
In the preceding code example, the WinMain function calls the CoCreateInstance function to create an instance of the IMMDeviceEnumerator interface, and it calls the IMMDeviceEnumerator::GetDefaultAudioEndpoint method to obtain the IMMDevice interface of the default rendering device. WinMain calls the IMMDevice::Activate method to obtain the device's IAudioMeterInformation interface, and it opens a dialog box to display a peak meter for the device. For more information about WinMain and CoCreateInstance, see the Windows SDK documentation. For more information about IMMDeviceEnumerator and IMMDevice, see Enumerating Audio Devices.

In the preceding code example, the DlgProc function displays the peak meter in the dialog box. During processing of the WM_INITDIALOG message, DlgProc calls the SetTimer function to set up a timer that will generate WM_TIMER messages at regular time intervals. When DlgProc receives a WM_TIMER message, it calls IAudioMeterInformation::GetPeakValue to obtain the latest peak-meter reading for the stream. DlgProc then calls the DrawPeakMeter function to draw the updated peak meter in the dialog box. For more information about SetTimer and the WM_INITDIALOG and WM_TIMER messages, see the Windows SDK documentation.

You can easily modify the preceding code example to display a peak meter for the default capture device. In the WinMain function, change the value of the first parameter in the call to the IMMDeviceEnumerator::GetDefaultAudioEndpoint from eRender to eCapture.

The following code example is the resource script that defines the controls that appear in the preceding code example:

C++

Copy
// Peakmeter.rc -- Resource script

#include "resource.h"
#include "windows.h"

//
// Dialog
//
PEAKMETER DIALOGEX 0, 0, 150, 34
STYLE DS_MODALFRAME | WS_CAPTION | WS_SYSMENU | DS_SETFONT
CAPTION "Peak Meter"
FONT 8, "Arial Rounded MT Bold", 400, 0, 0x0
BEGIN
    CTEXT      "",IDC_PEAK_METER,34,14,82,5
    LTEXT      "Min",IDC_STATIC_MINVOL,10,12,20,12
    RTEXT      "Max",IDC_STATIC_MAXVOL,120,12,20,12
END
The following code example is the resource header file that defines the control identifiers that appear in the preceding code examples:

C++

Copy
// Resource.h -- Control identifiers

#define IDC_STATIC_MINVOL      1001
#define IDC_STATIC_MAXVOL      1002
#define IDC_PEAK_METER         1003


After enumerating the audio endpoint devices in the system and identifying a suitable rendering or capture device, the next task for an audio client application is to open a connection with the endpoint device and to manage the flow of audio data over that connection. WASAPI enables clients to create and manage audio streams.

WASAPI implements several interfaces to provide stream-management services to audio clients. The primary interface is IAudioClient. A client obtains the IAudioClient interface for an audio endpoint device by calling the IMMDevice::Activate method (with parameter iid set to REFIID IID_IAudioClient) on the endpoint object.

The client calls the methods in the IAudioClient interface to do the following:

Discover which audio formats the endpoint device supports.
Get the endpoint buffer size.
Get the stream format and latency.
Start, stop, and reset the stream that flows through the endpoint device.
Access additional audio services.
To create a stream, a client calls the IAudioClient::Initialize method. Through this method, the client specifies the data format for the stream, the size of the endpoint buffer, and whether the stream operates in shared or exclusive mode.

The remaining methods in the IAudioClient interface fall into two groups:

Methods that can be called only after the stream has been opened by IAudioClient::Initialize.
Methods that can be called at any time before or after the Initialize call.
The following methods can be called only after the call to IAudioClient::Initialize:

IAudioClient::GetBufferSize
IAudioClient::GetCurrentPadding
IAudioClient::GetService
IAudioClient::GetStreamLatency
IAudioClient::Reset
IAudioClient::Start
IAudioClient::Stop
The following methods can be called before or after the IAudioClient::Initialize call:

IAudioClient::GetDevicePeriod
IAudioClient::GetMixFormat
IAudioClient::IsFormatSupported
To access the additional audio client services, the client calls the IAudioClient::GetService method. Through this method, the client can obtain references to the following interfaces:

IAudioRenderClient

Writes rendering data to an audio-rendering endpoint buffer.

IAudioCaptureClient

Reads captured data from an audio-capture endpoint buffer.

IAudioSessionControl

Communicates with the audio session manager to configure and manage the audio session that is associated with the stream.

ISimpleAudioVolume

Controls the volume level of the audio session that is associated with the stream.

IChannelAudioVolume

Controls the volume levels of the individual channels in the audio session that is associated with the stream.

IAudioClock

Monitors the stream data rate and stream position.

In addition, WASAPI clients that require notification of session-related events should implement the following interface:

IAudioSessionEvents

To receive event notifications, the client passes a pointer to its IAudioSessionEvents interface to the IAudioSessionControl::RegisterAudioSessionNotification method as a call parameter.

Finally, a client might use a higher-level API to create an audio stream, but also require access to the session controls and volume controls for the session that contains the stream. A higher-level API typically does not provide this access. The client can obtain the controls for a particular session through the IAudioSessionManager interface. This interface enables the client to obtain the IAudioSessionControl and ISimpleAudioVolume interfaces for a session without requiring the client to use the IAudioClient interface to create a stream and to assign the stream to the session. A client obtains the IAudioSessionManager interface for an audio endpoint device by calling the IMMDevice::Activate method (with parameter iid set to REFIID IID_IAudioSessionManager) on the endpoint object.

The IAudioSessionControl, IAudioSessionEvents, and IAudioSessionManager interfaces are defined in header file Audiopolicy.h. All other WASAPI interfaces are defined in header file Audioclient.h.

The following sections describe how to use WASAPI to manage audio streams:

About WASAPI
Rendering a Stream
Capturing a Stream
Loopback Recording
Exclusive-Mode Streams
Recovering from an Invalid-Device Error
Using a Communication Device
Stream Routing
The Windows Audio Session API (WASAPI) enables client applications to manage the flow of audio data between the application and an audio endpoint device.

The WASAPI interfaces are defined in the header files, Audioclient.h and Audiopolicy.h.

Every audio stream is a member of an audio session. Through the session abstraction, a WASAPI client can identify an audio stream as a member of a group of related audio streams. The system can manage all of the streams in the session as a single unit.

The audio engine is the user-mode audio component through which applications share access to an audio endpoint device. The audio engine transports audio data between an endpoint buffer and an endpoint device. To play an audio stream through a rendering endpoint device, an application periodically writes audio data to a rendering endpoint buffer. The audio engine mixes the streams from the various applications. To record an audio stream from a capture endpoint device, an application periodically reads audio data from a capture endpoint buffer.

WASAPI consists of several interfaces. The first of these is the IAudioClient interface. To access the WASAPI interfaces, a client first obtains a reference to the IAudioClient interface of an audio endpoint device by calling the IMMDevice::Activate method with parameter iid set to REFIID IID_IAudioClient. The client calls the IAudioClient::Initialize method to initialize a stream on an endpoint device. After initializing a stream, the client can obtain references to the other WASAPI interfaces by calling the IAudioClient::GetService method.

Many of the methods in WASAPI return error code AUDCLNT_E_DEVICE_INVALIDATED if the audio endpoint device that a client application is using becomes invalid. Frequently, the application can recover from this error. For more information, see Recovering from an Invalid-Device Error.

WASAPI implements the following interfaces.

Interface	Description
IAcousticEchoCancellationControl	Provides a mechanism for determining if an audio capture endpoint supports acoustic echo cancellation (AEC) and, if so, allows the client to set the audio render endpoint that should be used as the reference stream.
IAudioCaptureClient	Enables a client to read input data from a capture endpoint buffer.
IAudioClient	Enables a client to create and initialize an audio stream between an audio application and the audio engine or the hardware buffer of an audio endpoint device.
IAudioClient2	Enables a client to opt in for offloading, query stream properties, and get information from the hardware that handles offloading.
IAudioClient3	Enables a client to query for the audio engine's supported periodicities and current periodicity as well as request initialization of a shared audio stream with a specified periodicity.
IAudioClientDuckingControl	Provides a method that allows an app to specify that the system shouldn't duck the audio of other streams when the app's audio render stream is active.
IAudioClock	Enables a client to monitor a stream's data rate and the current position in the stream.
IAudioClock2	Enables a client to get the current device position.
IAudioClockAdjustment	Enables a client to adjust the sample rate of a stream.
IAudioEffectsManager	Provides management functionality for the audio effects pipeline.
IAudioRenderClient	Enables a client to write output data to a rendering endpoint buffer.
IAudioSessionControl	Enables a client to configure the control parameters for an audio session and to monitor events in the session.
IAudioSessionManager	Enables a client to access the session controls and volume controls for both cross-process and process-specific audio sessions.
IAudioStreamVolume	Enables a client to control and monitor the volume levels for all of the channels in an audio stream.
IAudioViewManagerService	Provides APIs for associating an HWND with an audio stream.
IChannelAudioVolume	Enables a client to control the volume levels for all of the channels in the audio session that the stream belongs to.
ISimpleAudioVolume	Enables a client to control the master volume level of an audio session.
WASAPI clients that require notification of session-related events should implement the following interface.

Interface	Description
IAudioEffectsChangedNotificationClient	A callback interface that allows applications to receive notifications when the list of audio effects changes or the resources needed to enable an effect changes.
IAudioSessionEvents	Provides notifications of session-related events such as changes in the volume level, display name, and session state.

The client calls the methods in the IAudioCaptureClient interface to read captured data from an endpoint buffer. The client shares the endpoint buffer with the audio engine in shared mode and with the audio device in exclusive mode. To request an endpoint buffer of a particular size, the client calls the IAudioClient::Initialize method. To get the size of the allocated buffer, which might be different from the requested size, the client calls the IAudioClient::GetBufferSize method.

To move a stream of captured data through the endpoint buffer, the client alternately calls the IAudioCaptureClient::GetBuffer method and the IAudioCaptureClient::ReleaseBuffer method. The client accesses the data in the endpoint buffer as a series of data packets. The GetBuffer call retrieves the next packet of captured data from the buffer. After reading the data from the packet, the client calls ReleaseBuffer to release the packet and make it available for more captured data.

The packet size can vary from one GetBuffer call to the next. Before calling GetBuffer, the client has the option of calling the IAudioCaptureClient::GetNextPacketSize method to get the size of the next packet in advance. In addition, the client can call the IAudioClient::GetCurrentPadding method to get the total amount of captured data that is available in the buffer. At any instant, the packet size is always less than or equal to the total amount of captured data in the buffer.

During each processing pass, the client has the option of processing the captured data in one of the following ways:

The client alternately calls GetBuffer and ReleaseBuffer, reading one packet with each pair of calls, until GetBuffer returns AUDCNT_S_BUFFEREMPTY, indicating that the buffer is empty.
The client calls GetNextPacketSize before each pair of calls to GetBuffer and ReleaseBuffer until GetNextPacketSize reports a packet size of 0, indicating that the buffer is empty.
The two techniques yield equivalent results.

The following code example shows how to record an audio stream from the default capture device:

C++

Copy
//-----------------------------------------------------------
// Record an audio stream from the default audio capture
// device. The RecordAudioStream function allocates a shared
// buffer big enough to hold one second of PCM audio data.
// The function uses this buffer to stream data from the
// capture device. The main loop runs every 1/2 second.
//-----------------------------------------------------------

// REFERENCE_TIME time units per second and per millisecond
#define REFTIMES_PER_SEC  10000000
#define REFTIMES_PER_MILLISEC  10000

#define EXIT_ON_ERROR(hres)  \
              if (FAILED(hres)) { goto Exit; }
#define SAFE_RELEASE(punk)  \
              if ((punk) != NULL)  \
                { (punk)->Release(); (punk) = NULL; }

const CLSID CLSID_MMDeviceEnumerator = __uuidof(MMDeviceEnumerator);
const IID IID_IMMDeviceEnumerator = __uuidof(IMMDeviceEnumerator);
const IID IID_IAudioClient = __uuidof(IAudioClient);
const IID IID_IAudioCaptureClient = __uuidof(IAudioCaptureClient);

HRESULT RecordAudioStream(MyAudioSink *pMySink)
{
    HRESULT hr;
    REFERENCE_TIME hnsRequestedDuration = REFTIMES_PER_SEC;
    REFERENCE_TIME hnsActualDuration;
    UINT32 bufferFrameCount;
    UINT32 numFramesAvailable;
    IMMDeviceEnumerator *pEnumerator = NULL;
    IMMDevice *pDevice = NULL;
    IAudioClient *pAudioClient = NULL;
    IAudioCaptureClient *pCaptureClient = NULL;
    WAVEFORMATEX *pwfx = NULL;
    UINT32 packetLength = 0;
    BOOL bDone = FALSE;
    BYTE *pData;
    DWORD flags;

    hr = CoCreateInstance(
           CLSID_MMDeviceEnumerator, NULL,
           CLSCTX_ALL, IID_IMMDeviceEnumerator,
           (void**)&pEnumerator);
    EXIT_ON_ERROR(hr)

    hr = pEnumerator->GetDefaultAudioEndpoint(
                        eCapture, eConsole, &pDevice);
    EXIT_ON_ERROR(hr)

    hr = pDevice->Activate(
                    IID_IAudioClient, CLSCTX_ALL,
                    NULL, (void**)&pAudioClient);
    EXIT_ON_ERROR(hr)

    hr = pAudioClient->GetMixFormat(&pwfx);
    EXIT_ON_ERROR(hr)

    hr = pAudioClient->Initialize(
                         AUDCLNT_SHAREMODE_SHARED,
                         0,
                         hnsRequestedDuration,
                         0,
                         pwfx,
                         NULL);
    EXIT_ON_ERROR(hr)

    // Get the size of the allocated buffer.
    hr = pAudioClient->GetBufferSize(&bufferFrameCount);
    EXIT_ON_ERROR(hr)

    hr = pAudioClient->GetService(
                         IID_IAudioCaptureClient,
                         (void**)&pCaptureClient);
    EXIT_ON_ERROR(hr)

    // Notify the audio sink which format to use.
    hr = pMySink->SetFormat(pwfx);
    EXIT_ON_ERROR(hr)

    // Calculate the actual duration of the allocated buffer.
    hnsActualDuration = (double)REFTIMES_PER_SEC *
                     bufferFrameCount / pwfx->nSamplesPerSec;

    hr = pAudioClient->Start();  // Start recording.
    EXIT_ON_ERROR(hr)

    // Each loop fills about half of the shared buffer.
    while (bDone == FALSE)
    {
        // Sleep for half the buffer duration.
        Sleep(hnsActualDuration/REFTIMES_PER_MILLISEC/2);

        hr = pCaptureClient->GetNextPacketSize(&packetLength);
        EXIT_ON_ERROR(hr)

        while (packetLength != 0)
        {
            // Get the available data in the shared buffer.
            hr = pCaptureClient->GetBuffer(
                                   &pData,
                                   &numFramesAvailable,
                                   &flags, NULL, NULL);
            EXIT_ON_ERROR(hr)

            if (flags & AUDCLNT_BUFFERFLAGS_SILENT)
            {
                pData = NULL;  // Tell CopyData to write silence.
            }

            // Copy the available capture data to the audio sink.
            hr = pMySink->CopyData(
                              pData, numFramesAvailable, &bDone);
            EXIT_ON_ERROR(hr)

            hr = pCaptureClient->ReleaseBuffer(numFramesAvailable);
            EXIT_ON_ERROR(hr)

            hr = pCaptureClient->GetNextPacketSize(&packetLength);
            EXIT_ON_ERROR(hr)
        }
    }

    hr = pAudioClient->Stop();  // Stop recording.
    EXIT_ON_ERROR(hr)

Exit:
    CoTaskMemFree(pwfx);
    SAFE_RELEASE(pEnumerator)
    SAFE_RELEASE(pDevice)
    SAFE_RELEASE(pAudioClient)
    SAFE_RELEASE(pCaptureClient)

    return hr;
}
In the preceding example, the RecordAudioStream function takes a single parameter, pMySink, which is a pointer to an object that belongs to a client-defined class, MyAudioSink, with two functions, CopyData and SetFormat. The example code does not include the implementation of MyAudioSink because:

None of the class members communicates directly with any of the methods in the interfaces in WASAPI.
The class could be implemented in a variety of ways, depending on the requirements of the client. (For example, it might write the capture data to a WAV file.)
However, information about the operation of the two methods is useful for understanding the example.

The CopyData function copies a specified number of audio frames from a specified buffer location. The RecordAudioStream function uses the CopyData function to read and save the audio data from the shared buffer. The SetFormat function specifies the format for the CopyData function to use for the data.

As long as the MyAudioSink object requires additional data, the CopyData function outputs the value FALSE through its third parameter, which, in the preceding code example, is a pointer to the variable bDone. When the MyAudioSink object has all the data that it requires, the CopyData function sets bDone to TRUE, which causes the program to exit the loop in the RecordAudioStream function.

The RecordAudioStream function allocates a shared buffer that has a duration of one second. (The allocated buffer might have a slightly longer duration.) Within the main loop, the call to the Windows Sleep function causes the program to wait for a half second. At the start of each Sleep call, the shared buffer is empty or nearly empty. By the time the Sleep call returns, the shared buffer is about half filled with capture data.

Following the call to the IAudioClient::Initialize method, the stream remains open until the client releases all of its references to the IAudioClient interface and to all references to service interfaces that the client obtained through the IAudioClient::GetService method. The final Release call closes the stream.

The client calls the methods in the IAudioRenderClient interface to write rendering data to an endpoint buffer. For a shared-mode stream, the client shares the endpoint buffer with the audio engine. For an exclusive-mode stream, the client shares the endpoint buffer with the audio device. To request an endpoint buffer of a particular size, the client calls the IAudioClient::Initialize method. To get the size of the allocated buffer, which might be different from the requested size, the client calls the IAudioClient::GetBufferSize method.

To move a stream of rendering data through the endpoint buffer, the client alternately calls the IAudioRenderClient::GetBuffer method and the IAudioRenderClient::ReleaseBuffer method. The client accesses the data in the endpoint buffer as a series of data packets. The GetBuffer call retrieves the next packet so that the client can fill it with rendering data. After writing the data to the packet, the client calls ReleaseBuffer to add the completed packet to the rendering queue.

For a rendering buffer, the padding value that is reported by the IAudioClient::GetCurrentPadding method represents the amount of rendering data that is queued up to play in the buffer. A rendering application can use the padding value to determine how much new data it can safely write to the buffer without the risk of overwriting previously written data that the audio engine has not yet read from the buffer. The available space is simply the buffer size minus the padding size. The client can request a packet size that represents some or all of this available space in its next GetBuffer call.

The size of a packet is expressed in audio frames. An audio frame in a PCM stream is a set of samples (the set contains one sample for each channel in the stream) that play or are recorded at the same time (clock tick). Thus, the size of an audio frame is the sample size multiplied by the number of channels in the stream. For example, the frame size for a stereo (2-channel) stream with 16-bit samples is four bytes.

The following code example shows how to play an audio stream on the default rendering device:

C++

Copy
//-----------------------------------------------------------
// Play an audio stream on the default audio rendering
// device. The PlayAudioStream function allocates a shared
// buffer big enough to hold one second of PCM audio data.
// The function uses this buffer to stream data to the
// rendering device. The inner loop runs every 1/2 second.
//-----------------------------------------------------------

// REFERENCE_TIME time units per second and per millisecond
#define REFTIMES_PER_SEC  10000000
#define REFTIMES_PER_MILLISEC  10000

#define EXIT_ON_ERROR(hres)  \
              if (FAILED(hres)) { goto Exit; }
#define SAFE_RELEASE(punk)  \
              if ((punk) != NULL)  \
                { (punk)->Release(); (punk) = NULL; }

const CLSID CLSID_MMDeviceEnumerator = __uuidof(MMDeviceEnumerator);
const IID IID_IMMDeviceEnumerator = __uuidof(IMMDeviceEnumerator);
const IID IID_IAudioClient = __uuidof(IAudioClient);
const IID IID_IAudioRenderClient = __uuidof(IAudioRenderClient);

HRESULT PlayAudioStream(MyAudioSource *pMySource)
{
    HRESULT hr;
    REFERENCE_TIME hnsRequestedDuration = REFTIMES_PER_SEC;
    REFERENCE_TIME hnsActualDuration;
    IMMDeviceEnumerator *pEnumerator = NULL;
    IMMDevice *pDevice = NULL;
    IAudioClient *pAudioClient = NULL;
    IAudioRenderClient *pRenderClient = NULL;
    WAVEFORMATEX *pwfx = NULL;
    UINT32 bufferFrameCount;
    UINT32 numFramesAvailable;
    UINT32 numFramesPadding;
    BYTE *pData;
    DWORD flags = 0;

    hr = CoCreateInstance(
           CLSID_MMDeviceEnumerator, NULL,
           CLSCTX_ALL, IID_IMMDeviceEnumerator,
           (void**)&pEnumerator);
    EXIT_ON_ERROR(hr)

    hr = pEnumerator->GetDefaultAudioEndpoint(
                        eRender, eConsole, &pDevice);
    EXIT_ON_ERROR(hr)

    hr = pDevice->Activate(
                    IID_IAudioClient, CLSCTX_ALL,
                    NULL, (void**)&pAudioClient);
    EXIT_ON_ERROR(hr)

    hr = pAudioClient->GetMixFormat(&pwfx);
    EXIT_ON_ERROR(hr)

    hr = pAudioClient->Initialize(
                         AUDCLNT_SHAREMODE_SHARED,
                         0,
                         hnsRequestedDuration,
                         0,
                         pwfx,
                         NULL);
    EXIT_ON_ERROR(hr)

    // Tell the audio source which format to use.
    hr = pMySource->SetFormat(pwfx);
    EXIT_ON_ERROR(hr)

    // Get the actual size of the allocated buffer.
    hr = pAudioClient->GetBufferSize(&bufferFrameCount);
    EXIT_ON_ERROR(hr)

    hr = pAudioClient->GetService(
                         IID_IAudioRenderClient,
                         (void**)&pRenderClient);
    EXIT_ON_ERROR(hr)

    // Grab the entire buffer for the initial fill operation.
    hr = pRenderClient->GetBuffer(bufferFrameCount, &pData);
    EXIT_ON_ERROR(hr)

    // Load the initial data into the shared buffer.
    hr = pMySource->LoadData(bufferFrameCount, pData, &flags);
    EXIT_ON_ERROR(hr)

    hr = pRenderClient->ReleaseBuffer(bufferFrameCount, flags);
    EXIT_ON_ERROR(hr)

    // Calculate the actual duration of the allocated buffer.
    hnsActualDuration = (double)REFTIMES_PER_SEC *
                        bufferFrameCount / pwfx->nSamplesPerSec;

    hr = pAudioClient->Start();  // Start playing.
    EXIT_ON_ERROR(hr)

    // Each loop fills about half of the shared buffer.
    while (flags != AUDCLNT_BUFFERFLAGS_SILENT)
    {
        // Sleep for half the buffer duration.
        Sleep((DWORD)(hnsActualDuration/REFTIMES_PER_MILLISEC/2));

        // See how much buffer space is available.
        hr = pAudioClient->GetCurrentPadding(&numFramesPadding);
        EXIT_ON_ERROR(hr)

        numFramesAvailable = bufferFrameCount - numFramesPadding;

        // Grab all the available space in the shared buffer.
        hr = pRenderClient->GetBuffer(numFramesAvailable, &pData);
        EXIT_ON_ERROR(hr)

        // Get next 1/2-second of data from the audio source.
        hr = pMySource->LoadData(numFramesAvailable, pData, &flags);
        EXIT_ON_ERROR(hr)

        hr = pRenderClient->ReleaseBuffer(numFramesAvailable, flags);
        EXIT_ON_ERROR(hr)
    }

    // Wait for last data in buffer to play before stopping.
    Sleep((DWORD)(hnsActualDuration/REFTIMES_PER_MILLISEC/2));

    hr = pAudioClient->Stop();  // Stop playing.
    EXIT_ON_ERROR(hr)

Exit:
    CoTaskMemFree(pwfx);
    SAFE_RELEASE(pEnumerator)
    SAFE_RELEASE(pDevice)
    SAFE_RELEASE(pAudioClient)
    SAFE_RELEASE(pRenderClient)

    return hr;
}
In the preceding example, the PlayAudioStream function takes a single parameter, pMySource, which is a pointer to an object that belongs to a client-defined class, MyAudioSource, with two member functions, LoadData and SetFormat. The example code does not include the implementation of MyAudioSource because:

None of the class members communicates directly with any of the methods in the interfaces in WASAPI.
The class could be implemented in a variety of ways, depending on the requirements of the client. (For example, it might read the rendering data from a WAV file and perform on-the-fly conversion to the stream format.)
However, some information about the operation of the two functions is useful for understanding the example.

The LoadData function writes a specified number of audio frames (first parameter) to a specified buffer location (second parameter). (The size of an audio frame is the number of channels in the stream multiplied by the sample size.) The PlayAudioStream function uses LoadData to fill portions of the shared buffer with audio data. The SetFormat function specifies the format for the LoadData function to use for the data. If the LoadData function is able to write at least one frame to the specified buffer location but runs out of data before it has written the specified number of frames, then it writes silence to the remaining frames.

As long as LoadData succeeds in writing at least one frame of real data (not silence) to the specified buffer location, it outputs 0 through its third parameter, which, in the preceding code example, is an output pointer to the flags variable. When LoadData is out of data and cannot write even a single frame to the specified buffer location, it writes nothing to the buffer (not even silence), and it writes the value AUDCLNT_BUFFERFLAGS_SILENT to the flags variable. The flags variable conveys this value to the IAudioRenderClient::ReleaseBuffer method, which responds by filling the specified number of frames in the buffer with silence.

In its call to the IAudioClient::Initialize method, the PlayAudioStream function in the preceding example requests a shared buffer that has a duration of one second. (The allocated buffer might have a slightly longer duration.) In its initial calls to the IAudioRenderClient::GetBuffer and IAudioRenderClient::ReleaseBuffer methods, the function fills the entire buffer before calling the IAudioClient::Start method to begin playing the buffer.

Within the main loop, the function iteratively fills half of the buffer at half-second intervals. Just before each call to the Windows Sleep function in the main loop, the buffer is full or nearly full. When the Sleep call returns, the buffer is about half full. The loop ends after the final call to the LoadData function sets the flags variable to the value AUDCLNT_BUFFERFLAGS_SILENT. At that point, the buffer contains at least one frame of real data, and it might contain as much as a half second of real data. The remainder of the buffer contains silence. The Sleep call that follows the loop provides sufficient time (a half second) to play all of the remaining data. The silence that follows the data prevents unwanted sounds before the call to the IAudioClient::Stop method stops the audio stream. For more information about Sleep, see the Windows SDK documentation.

Following the call to the IAudioClient::Initialize method, the stream remains open until the client releases all of its references to the IAudioClient interface and to all references to service interfaces that the client obtained through the IAudioClient::GetService method. The final Release call closes the stream.

The PlayAudioStream function in the preceding code example calls the CoCreateInstance function to create an enumerator for the audio endpoint devices in the system. Unless the calling program previously called either the CoCreateInstance or CoInitializeEx function to initialize the COM library, the CoCreateInstance call will fail. For more information about CoCreateInstance, CoCreateInstance, and CoInitializeEx, see the Windows SDK documentation.

Exclusive-Mode Streams
06/14/2021
As explained previously, if an application opens a stream in exclusive mode, the application has exclusive use of the audio endpoint device that plays or records the stream. In contrast, several applications can share an audio endpoint device by opening shared-mode streams on the device.

Exclusive-mode access to an audio device can block crucial system sounds, prevent interoperability with other applications, and otherwise degrade the user experience. To mitigate these problems, an application with an exclusive-mode stream typically relinquishes control of the audio device when the application is not the foreground process or is not actively streaming.

Stream latency is the delay that is inherent in the data path that connects an application's endpoint buffer with an audio endpoint device. For a rendering stream, the latency is the maximum delay from the time that an application writes a sample to an endpoint buffer to the time that the sample is heard through the speakers. For a capture stream, the latency is the maximum delay from the time that a sound enters the microphone to the time that an application can read the sample for that sound from the endpoint buffer.

Applications that use exclusive-mode streams often do so because they require low latencies in the data paths between the audio endpoint devices and the application threads that access the endpoint buffers. Typically, these threads run at relatively high priority and schedule themselves to run at periodic intervals that are close to or the same as the periodic interval that separates successive processing passes by the audio hardware. During each pass, the audio hardware processes the new data in the endpoint buffers.

To achieve the smallest stream latencies, an application might require both special audio hardware, and a computer system that is lightly loaded. Driving the audio hardware beyond its timing limits or loading the system with competing high-priority tasks might cause a glitch in a low-latency audio stream. For example, for a rendering stream, a glitch can occur if the application fails to write to an endpoint buffer before the audio hardware reads the buffer, or if the hardware fails to read the buffer before the time that the buffer is scheduled to play. Typically, an application that is intended to run on a wide variety of audio hardware and in a broad range of systems should relax its timing requirements enough to avoid glitches in all target environments.

Windows Vista has several features to support applications that require low-latency audio streams. As discussed in User-Mode Audio Components, applications that perform time-critical operations can call the Multimedia Class Scheduler Service (MMCSS) functions to increase thread priority without denying CPU resources to lower-priority applications. In addition, the IAudioClient::Initialize method supports an AUDCLNT_STREAMFLAGS_EVENTCALLBACK flag that enables an application's buffer-servicing thread to schedule its execution to occur when a new buffer becomes available from the audio device. By using these features, an application thread can reduce uncertainty about when it will execute, thereby decreasing the risk of glitches in a low-latency audio stream.

The drivers for older audio adapters are likely to use the WaveCyclic or WavePci device driver interface (DDI), whereas the drivers for newer audio adapters are more likely to support the WaveRT DDI. For exclusive-mode applications, WaveRT drivers can provide better performance than WaveCyclic or WavePci drivers, but WaveRT drivers require additional hardware capabilities. These capabilities include the ability to share hardware buffers directly with applications. With direct sharing, no system intervention is required to transfer data between an exclusive-mode application and the audio hardware. In contrast, WaveCyclic and WavePci drivers are suitable for older, less capable audio adapters. These adapters rely on system software to transport blocks of data (attached to system I/O request packets, or IRPs) between application buffers and hardware buffers. In addition, USB audio devices rely on system software to transport data between application buffers and hardware buffers. To improve the performance of exclusive-mode applications that connect to audio devices that rely on the system for data transport, WASAPI automatically increases the priority of the system threads that transfer data between the applications and the hardware. WASAPI uses MMCSS to increase the thread priority. In Windows Vista, if a system thread manages data transport for an exclusive-mode audio playback stream with a PCM format and a device period of less than 10 milliseconds, WASAPI assigns the MMCSS task name "Pro Audio" to the thread. If the device period of the stream is greater than or equal to 10 milliseconds, WASAPI assigns the MMCSS task name "Audio" to the thread. For more information about the WaveCyclic, WavePci, and WaveRT DDIs, see the Windows DDK documentation. For information about selecting an appropriate device period, see IAudioClient::GetDevicePeriod.

As described in Session Volume Controls, WASAPI provides the ISimpleAudioVolume, IChannelAudioVolume, and IAudioStreamVolume interfaces for controlling the volume levels of shared-mode audio streams. However, the controls in these interfaces have no effect on exclusive-mode streams. Instead, applications that manage exclusive-mode streams typically use the IAudioEndpointVolume interface in the EndpointVolume API to control the volume levels of those streams. For information about this interface, see Endpoint Volume Controls.

For each playback device and capture device in the system, the user can control whether the device can be used in exclusive mode. If the user disables exclusive-mode use of the device, the device can be used to play or record only shared-mode streams.

If the user enables exclusive-mode use of the device, the user can also control whether a request by an application to use the device in exclusive mode will preempt the use of the device by applications that might currently be playing or recording shared-mode streams through the device. If preemption is enabled, a request by an application to take exclusive control of the device succeeds if the device is currently not in use, or if the device is being used in shared mode, but the request fails if another application already has exclusive control of the device. If preemption is disabled, a request by an application to take exclusive control of the device succeeds if the device is not currently in use, but the request fails if the device is already being used either in shared mode or in exclusive mode.

In Windows Vista, the default settings for an audio endpoint device are the following:

The device can be used to play or record exclusive-mode streams.
A request to use a device to play or record an exclusive-mode stream preempts any shared-mode stream that is currently being played or recorded through the device.
To change the exclusive-mode settings of a playback or recording device

Right-click the speaker icon in the notification area, which is located on the right side of the taskbar, and select Playback Devices or Recording Devices. (As an alternative, run the Windows multimedia control panel, Mmsys.cpl, from a Command Prompt window. For more information, see Remarks in DEVICE_STATE_XXX Constants.)
After the Sound window appears, select Playback or Recording. Next, select an entry in the list of device names, and click Properties.
After the Properties window appears, click Advanced.
To enable applications to use the device in exclusive mode, check the box labeled Allow applications to take exclusive control of this device. To disable exclusive-mode use of the device, clear the check box.
If exclusive-mode use of the device is enabled, you can specify whether a request for exclusive control of the device will succeed if the device is currently playing or recording shared-mode streams. To give exclusive-mode applications priority over shared-mode applications, check the box labeled Give exclusive mode applications priority. To deny exclusive-mode applications priority over shared-mode applications, clear the check box.
The following code example shows how to play a low-latency audio stream on an audio-rendering device that is configured for use in exclusive mode:

C++

Copy
//-----------------------------------------------------------
// Play an exclusive-mode stream on the default audio
// rendering device. The PlayExclusiveStream function uses
// event-driven buffering and MMCSS to play the stream at
// the minimum latency supported by the device.
//-----------------------------------------------------------

// REFERENCE_TIME time units per second and per millisecond
#define REFTIMES_PER_SEC  10000000
#define REFTIMES_PER_MILLISEC  10000

#define EXIT_ON_ERROR(hres)  \
              if (FAILED(hres)) { goto Exit; }
#define SAFE_RELEASE(punk)  \
              if ((punk) != NULL)  \
                { (punk)->Release(); (punk) = NULL; }

const CLSID CLSID_MMDeviceEnumerator = __uuidof(MMDeviceEnumerator);
const IID IID_IMMDeviceEnumerator = __uuidof(IMMDeviceEnumerator);
const IID IID_IAudioClient = __uuidof(IAudioClient);
const IID IID_IAudioRenderClient = __uuidof(IAudioRenderClient);

HRESULT PlayExclusiveStream(MyAudioSource *pMySource)
{
    HRESULT hr;
    REFERENCE_TIME hnsRequestedDuration = 0;
    IMMDeviceEnumerator *pEnumerator = NULL;
    IMMDevice *pDevice = NULL;
    IAudioClient *pAudioClient = NULL;
    IAudioRenderClient *pRenderClient = NULL;
    WAVEFORMATEX *pwfx = NULL;
    HANDLE hEvent = NULL;
    HANDLE hTask = NULL;
    UINT32 bufferFrameCount;
    BYTE *pData;
    DWORD flags = 0;
    DWORD taskIndex = 0;
    
    hr = CoCreateInstance(
           CLSID_MMDeviceEnumerator, NULL,
           CLSCTX_ALL, IID_IMMDeviceEnumerator,
           (void**)&pEnumerator);
    EXIT_ON_ERROR(hr)

    hr = pEnumerator->GetDefaultAudioEndpoint(
                        eRender, eConsole, &pDevice);
    EXIT_ON_ERROR(hr)

    hr = pDevice->Activate(
                    IID_IAudioClient, CLSCTX_ALL,
                    NULL, (void**)&pAudioClient);
    EXIT_ON_ERROR(hr)

    // Call a helper function to negotiate with the audio
    // device for an exclusive-mode stream format.
    hr = GetStreamFormat(pAudioClient, &pwfx);
    EXIT_ON_ERROR(hr)

    // Initialize the stream to play at the minimum latency.
    hr = pAudioClient->GetDevicePeriod(NULL, &hnsRequestedDuration);
    EXIT_ON_ERROR(hr)

    hr = pAudioClient->Initialize(
                         AUDCLNT_SHAREMODE_EXCLUSIVE,
                         AUDCLNT_STREAMFLAGS_EVENTCALLBACK,
                         hnsRequestedDuration,
                         hnsRequestedDuration,
                         pwfx,
                         NULL);
    if (hr == AUDCLNT_E_BUFFER_SIZE_NOT_ALIGNED) {
        // Align the buffer if needed, see IAudioClient::Initialize() documentation
        UINT32 nFrames = 0;
        hr = pAudioClient->GetBufferSize(&nFrames);
        EXIT_ON_ERROR(hr)
        hnsRequestedDuration = (REFERENCE_TIME)((double)REFTIMES_PER_SEC / pwfx->nSamplesPerSec * nFrames + 0.5);
        hr = pAudioClient->Initialize(
            AUDCLNT_SHAREMODE_EXCLUSIVE,
            AUDCLNT_STREAMFLAGS_EVENTCALLBACK,
            hnsRequestedDuration,
            hnsRequestedDuration,
            pwfx,
            NULL);
    }
    EXIT_ON_ERROR(hr)

    // Tell the audio source which format to use.
    hr = pMySource->SetFormat(pwfx);
    EXIT_ON_ERROR(hr)

    // Create an event handle and register it for
    // buffer-event notifications.
    hEvent = CreateEvent(NULL, FALSE, FALSE, NULL);
    if (hEvent == NULL)
    {
        hr = E_FAIL;
        goto Exit;
    }

    hr = pAudioClient->SetEventHandle(hEvent);
    EXIT_ON_ERROR(hr);

    // Get the actual size of the two allocated buffers.
    hr = pAudioClient->GetBufferSize(&bufferFrameCount);
    EXIT_ON_ERROR(hr)

    hr = pAudioClient->GetService(
                         IID_IAudioRenderClient,
                         (void**)&pRenderClient);
    EXIT_ON_ERROR(hr)

    // To reduce latency, load the first buffer with data
    // from the audio source before starting the stream.
    hr = pRenderClient->GetBuffer(bufferFrameCount, &pData);
    EXIT_ON_ERROR(hr)

    hr = pMySource->LoadData(bufferFrameCount, pData, &flags);
    EXIT_ON_ERROR(hr)

    hr = pRenderClient->ReleaseBuffer(bufferFrameCount, flags);
    EXIT_ON_ERROR(hr)

    // Ask MMCSS to temporarily boost the thread priority
    // to reduce glitches while the low-latency stream plays.
    hTask = AvSetMmThreadCharacteristics(TEXT("Pro Audio"), &taskIndex);
    if (hTask == NULL)
    {
        hr = E_FAIL;
        EXIT_ON_ERROR(hr)
    }

    hr = pAudioClient->Start();  // Start playing.
    EXIT_ON_ERROR(hr)

    // Each loop fills one of the two buffers.
    while (flags != AUDCLNT_BUFFERFLAGS_SILENT)
    {
        // Wait for next buffer event to be signaled.
        DWORD retval = WaitForSingleObject(hEvent, 2000);
        if (retval != WAIT_OBJECT_0)
        {
            // Event handle timed out after a 2-second wait.
            pAudioClient->Stop();
            hr = ERROR_TIMEOUT;
            goto Exit;
        }

        // Grab the next empty buffer from the audio device.
        hr = pRenderClient->GetBuffer(bufferFrameCount, &pData);
        EXIT_ON_ERROR(hr)

        // Load the buffer with data from the audio source.
        hr = pMySource->LoadData(bufferFrameCount, pData, &flags);
        EXIT_ON_ERROR(hr)

        hr = pRenderClient->ReleaseBuffer(bufferFrameCount, flags);
        EXIT_ON_ERROR(hr)
    }

    // Wait for the last buffer to play before stopping.
    Sleep((DWORD)(hnsRequestedDuration/REFTIMES_PER_MILLISEC));

    hr = pAudioClient->Stop();  // Stop playing.
    EXIT_ON_ERROR(hr)

Exit:
    if (hEvent != NULL)
    {
        CloseHandle(hEvent);
    }
    if (hTask != NULL)
    {
        AvRevertMmThreadCharacteristics(hTask);
    }
    CoTaskMemFree(pwfx);
    SAFE_RELEASE(pEnumerator)
    SAFE_RELEASE(pDevice)
    SAFE_RELEASE(pAudioClient)
    SAFE_RELEASE(pRenderClient)

    return hr;
}
In the preceding code example, the PlayExclusiveStream function runs in the application thread that services the endpoint buffers while a rendering stream is playing. The function takes a single parameter, pMySource, which is a pointer to an object that belongs to a client-defined class, MyAudioSource. This class has two member functions, LoadData and SetFormat, that are called in the code example. MyAudioSource is described in Rendering a Stream.

The PlayExclusiveStream function calls a helper function, GetStreamFormat, that negotiates with the default rendering device to determine whether the device supports an exclusive-mode stream format that is suitable for use by the application. The code for the GetStreamFormat function does not appear in the code example; that is because the details of its implementation depend entirely on the requirements of the application. However, the operation of the GetStreamFormat function can be described simply—it calls the IAudioClient::IsFormatSupported method one or more times to determine whether the device supports a suitable format. The requirements of the application dictate which formats GetStreamFormat presents to the IsFormatSupported method and the order in which it presents them. For more information about IsFormatSupported, see Device Formats.

After the GetStreamFormat call, the PlayExclusiveStream function calls the IAudioClient::GetDevicePeriod method to obtain the minimum device period supported by the audio hardware. Next, the function calls the IAudioClient::Initialize method to request a buffer duration equal to the minimum period. If the call succeeds, the Initialize method allocates two endpoint buffers, each of which is equal in duration to the minimum period. Later, when the audio stream begins running, the application and audio hardware will share the two buffers in "ping-pong" fashion—that is, while the application writes to one buffer, the hardware reads from the other buffer.

Before starting the stream, the PlayExclusiveStream function does the following:

Creates and registers the event handle through which it will receive notifications when buffers become ready to fill.
Fills the first buffer with data from the audio source to reduce the delay from when the stream starts running to when the initial sound is heard.
Calls the AvSetMmThreadCharacteristics function to request that MMCSS increase the priority of the thread in which PlayExclusiveStream executes. (When the stream stops running, the AvRevertMmThreadCharacteristics function call restores the original thread priority.)
For more information about AvSetMmThreadCharacteristics and AvRevertMmThreadCharacteristics, see the Windows SDK documentation.

While the stream is running, each iteration of the while-loop in the preceding code example fills one endpoint buffer. Between iterations, the WaitForSingleObject function call waits for the event handle to be signaled. When the handle is signaled, the loop body does the following:

Calls the IAudioRenderClient::GetBuffer method to get the next buffer.
Fills the buffer.
Calls the IAudioRenderClient::ReleaseBuffer method to release the buffer.
For more information about WaitForSingleObject, see the Windows SDK documentation.

If the audio adapter is controlled by a WaveRT driver, the signaling of the event handle is tied to the DMA-transfer notifications from the audio hardware. For a USB audio device, or for an audio device that is controlled by a WaveCyclic or WavePci driver, the signaling of the event handle is tied to completions of the IRPs that transfer data from the application buffer to the hardware buffer.

The preceding code example pushes the audio hardware and the computer system to their performance limits. First, to reduce the stream latency, the application schedules its buffer-servicing thread to use the minimum device period that the audio hardware will support. Second, to ensure that the thread reliably executes within each device period, the AvSetMmThreadCharacteristics function call sets the TaskName parameter to "Pro Audio", which is, in Windows Vista, the default task name with the highest priority. Consider whether the timing requirements of your application might be relaxed without compromising its usefulness. For example, the application might schedule its buffer-servicing thread to use a period that is longer than the minimum. A longer period might safely allow the use of a lower thread priority.

Loopback Recording
04/16/2025
In loopback mode, a client of WASAPI can capture the audio stream that is being played by a rendering endpoint device. To open a stream in loopback mode, the client must:

Obtain an IMMDevice interface for the rendering endpoint device.
Initialize a capture stream in loopback mode on the rendering endpoint device.
After following these steps, the client can call the IAudioClient::GetService method to obtain an IAudioCaptureClient interface on the rendering endpoint device.

WASAPI provides loopback mode primarily to support acoustic echo cancellation (AEC). However, other types of audio applications might find loopback mode useful for capturing the system mix that is being played by the audio engine.

In the code example in Capturing a Stream, the RecordAudioStream function can be easily modified to configure a loopback-mode capture stream. The required modifications are:

In the call to the IMMDeviceEnumerator::GetDefaultAudioEndpoint method, change the first parameter (dataFlow) from eCapture to eRender.
In the call to the IAudioClient::Initialize method, change the value of the second parameter (StreamFlags) from 0 to AUDCLNT_STREAMFLAGS_LOOPBACK.
In versions of Windows prior to Windows 10 1703, pull-mode capture client does not receive any events when a stream is initialized with event-driven buffering and is loopback-enabled. To work around this, initialize a render stream in event-driven mode. Each time the client receives an event for the render stream, it must signal the capture client to run the capture thread that reads the next set of samples from the capture endpoint buffer. In Windows 10 versions 1703 and higher, event-driven loopback clients are supported, and no longer need the workaround involving the render stream.

A client can enable loopback mode only for a shared-mode stream (AUDCLNT_SHAREMODE_SHARED). Exclusive-mode streams cannot operate in loopback mode.

The implementation of loopback by WASAPI depend on the capabilities of the hardware. If the hardware supports a loopback pin on the render endpoint, WASAPI uses the audio provided on this pin for the loopback stream. When the hardware does not support a loopback pin, WASAPI copies the output stream from the audio engine into the loopback application's capture buffer, in addition to copying the audio data to the hardware's render pin.

Some hardware vendors implement loopback devices (as opposed to pin instances on render devices) in their audio adapters. Although hardware loopback devices are similar in operation to the WASAPI loopback mode, they can be more difficult to use.

Hardware loopback devices have the following disadvantages for audio applications:

Not all audio adapters have loopback devices. Thus, applications that depend on them will not work on all systems.
Before an application can record from a loopback device, the user must identify the loopback device and enable it for use.
Different vendors assign different names to their hardware loopback devices. The following names are examples:

Stereo Mix
Waveout Mix
Mixed Output
What You Hear
The lack of standardized names might cause users to have difficulty identifying a loopback device in a list of device names.

A hardware loopback device is a capture device. Thus, if an adapter supports a loopback device, an audio application can record from the device in the same way that it records from any other capture device.

For example, if you select a hardware loopback device to be the default capture device, you can use the RecordAudioStream function (without modification) in the code example in Capturing a Stream to capture the stream from the device. (You can also use a legacy audio API, such as the Windows multimedia waveInXxx functions, to capture the stream from the device.)

If your audio adapter contains a hardware loopback device, you can use the Windows multimedia control panel, Mmsys.cpl, to designate the device as the default capture device. The steps are as follows:

To run Mmsys.cpl, open a Command Prompt window and enter the following command:

ps1

Copy
control mmsys.cpl
Alternatively, you can run Mmsys.cpl by right-clicking the speaker icon in the notification area, which is located on the right side of the taskbar, and selecting Recording Devices.

After the Mmsys.cpl window opens, right-click anywhere in the list of recording devices and verify that the Show Disabled Devices option is checked. (Otherwise, if the loopback device is disabled, it will not appear in the list.)

Browse the list of recording devices to locate the loopback device (if it exists). If the loopback device is disabled, enable it by right-clicking the device and clicking Enable.

Finally, to select the loopback device to be the default capture device, right-click the device and click Set as Default Device.

WASAPI supports loopback recording regardless of whether the audio hardware contains a loopback device, or whether the user has enabled the device.

Windows Vista provides digital rights management (DRM). Content providers rely on DRM to protect their proprietary music or other content from unauthorized copying and other illegal uses. Similarly, a trusted audio driver does not permit a loopback device to capture digital streams that contain protected content. Windows Vista allows only trusted drivers to play protected content. For more information about trusted drivers and DRM, see the Windows DDK documentation.

WASAPI loopback by default contains the mix of all audio being played, regardless of the Terminal Services session the audio originated from. For example, you can run a loopback client in a service running in session 0 and capture audio from all user sessions, as well as audio being played from session 0.

Remote Desktop allows redirecting audio to the client. This is implemented by creating new audio devices that only appear for that session.

Using a Communication Device
01/06/2021
In Windows 7, the Windows multimedia control panel, Mmsys.cpl, provides a new Communications tab. This tab contains options that enables a user to set options that defines how the system manages a communication device. A communication device is used primarily for placing or receiving telephone calls on the computer. For a computer that has only one rendering device (speaker) and one capture device (microphone), these audio devices also act as the default communication devices. When a user connects a new device, such as a USB headset, the system performs automatic device role detection by looking up the configuration settings that are populated by the OEM. If the system determines a device to be best suited for communication purposes, the system assigns the eCommunications role to the device. For these devices, the Windows 7 Mmsys.cpl provides the option Default Communication Device that enables a user to select a communication device each for audio rendering (Playback tab) and audio capturing (Recording tab). The system performs automatic role detection but does not set a particular device to be used for communications. This must be done by the user. The new eCommunications role lets an application distinguish between a device that is chosen by the user for handling phone calls and a device to be used as a multimedia device (music playback). For example, if the user has a headset and a speaker connected to the computer, the system assigns the eConsole role to the speaker and the eCommunications role to the headset. After the user selects the headset to be used as the communication device, to develop a communication application, you can target the headset specifically for rendering an audio stream. An application the user cannot change the device role assigned by the system. For more information about device roles, see Device Roles.

Communication applications, such as VoIP and Unified Communication applications, place and receive phone calls through a computer. For example, a VoIP application might assign a stream that contains the ring-in notification to the endpoint of a communication device set for rendering audio streams. In addition, the application might open the voice input and output streams on the capture and rendering endpoint devices that are set as communication devices.

To integrate communication capabilities into your applications, you can use:

MMDevice API—to get a reference to the endpoint of the communication device.
WASAPI—to render and capture audio streams through the communication device. The operating system considers the stream opened on a communication device to be a communication stream.
The communication application enumerates devices and provides stream management for a communication stream (rendering or capture) stream in the same manner as it would manage a non-communication stream by using the Core Audio APIs.

One of the features that you can integrate in your communication application is ducking or stream attenuation. This behavior defines what must happen to other sounds when a communication stream is opened, such as when a phone call is received on the communication device. The system might mute or lower the audio volume of the non-communication stream depending on the user's choice. The audio system generates ducking events when a communication stream is opened or closed for rendering or capturing streams. By default, the operating system provides a default ducking experience. A media application can replace the default behavior and handle these events itself to provide a customized ducking experience.

The following sections describe how to use Core Audio APIs to provide a custom ducking experience.

Default Ducking Experience
Disabling the Default Ducking Experience
Providing a Custom Ducking Behavior
Implementation Considerations for Ducking Notifications
Getting Ducking Events
Getting a Reference to the Communication Device Endpoint
To use the communication device, a direct WASAPI client must enumerate the devices by using the device enumerator. Get a reference to the endpoint of the default communication device by calling IMMDeviceEnumerator::GetDefaultAudioEndpoint. In this call, the application must specify eCommunications in the Role parameter to restrict the device enumeration to communication devices. After you get a reference to the device endpoint for the device, you can activate the services that are scoped for the endpoint by calling IMMDevice::Activate. For example, you can pass the IID_IAudioClient service identifier to activate an audio client object and use it for stream management, the IID_IAudioEndpointVolume identifier to get access to the volume controls of the communication device endpoint, or the IID_IAudioSessionManager identifier to activate the session manager that enables you to interact with the policy engine of the endpoint. For information about stream operations, see Stream Management.

By using the IMMDevice reference, you can also access the property store for the device endpoint. These property values, such as device friendly name and manufacturer name, are populated by the OEM and enable an application to determine the characteristics of a communication device. For more information, see Device Properties.

The following example code gets a reference to the endpoint of the default communication device for rendering an audio stream.

C++

Copy
IMMDevice *defaultDevice = NULL;

hr = CoCreateInstance(__uuidof(MMDeviceEnumerator), NULL,
            CLSCTX_INPROC_SERVER, 
            __uuidof(IMMDeviceEnumerator), 
            (LPVOID *)&deviceEnumerator);

hr = deviceEnumerator->GetDefaultAudioEndpoint(eRender, 
            eCommunications, &defaultDevice);

            Stream Routing
01/06/2021
Stream routing is the ability of a media application to switch streams between devices with minimal interruption to the playback or the capture session.

A computer can have multiple rendering and capture devices. The system lists these devices on the Sounds control panel. From this list, a user can set a device to be the default device for each role: playback, recording, or the four communications roles (console render, console capture, communication render, or communication capture). The list of devices may be modified dynamically as some of these devices can be available temporarily, for example a USB headset. When multiple devices are available, the user can change the default to a different device. The user can also change the format of a device (sample rate, bits per sample, and so on) on the Advanced tab for the device properties.

Consider a scenario in which a user selects Speakers as the default device for rendering audio streams. The user then connects a USB headset, selects the headset as the new default device, and changes the sample rate of the device from 44.1 kHz to 48 kHz. The user wants to play the audio stream on the headset at the new sample rate with minimal interruption to the streaming session.

In this scenario, there are two cases that the media application must handle:

The stream must be transferred to the new default device with minimal interruption to playback.
The new device must resume playback in the new format (that is, the user can change more than the sample rate).
In Windows Vista, to support this scenario, the media application had to provide the implementation for stream routing. The application was responsible for terminating existing streams and restarting the streams on the new device. If the user changed the default device or its mix format changed, then all of the associated sessions were closed and the application had to handle the recovery.

In Windows 7, an application can seamlessly transfer a stream from an existing default device to a new default audio endpoint. High-level audio API sets such as Media Foundation, DirectSound, and WAVE APIs implement the stream routing feature. Media applications that use these API sets to play or capture a stream from the default device use the default implementation and will not have to modify the application. However, if your media application uses MMDeviceAPI or WASAPI directly, the application needs to provide the stream routing implementation.

 Note

MMDeviceAPI and WASAPI are Core Audio API components that an application can use to render or capture a stream on a device. The MMDeviceAPI discovers the new audio endpoint device, and WASAPI manages the flow of audio data between a media application and the audio endpoint device.

 

To implement the stream routing feature, the application must listen for the notifications sent by MMDeviceAPI and WASAPI when:

The default device is changed by the user.
The existing default device is removed and a new default device is added.
The device format is changed.
By handling these notifications, an application can perform the necessary stream management operations while transferring the stream to the new default device. In addition, the application can render or capture existing streams by using the new format specified by the user while a rendering session is active.

This section contains the following topics:

Getting the Device Endpoint for Stream Routing
Relevant Notifications for Stream Routing
Stream Routing Implementation Considerations
The following samples, included in the Windows SDK, demonstrate how an application can handle stream routing notifications.

RenderSharedTimerDriven
RenderSharedEventDriven
RenderExclusiveTimerDriven
RenderExclusiveEventDriven
CaptureSharedTimerDriven
CaptureSharedEventDriven

Getting the Device Endpoint for Stream Routing
06/09/2025
In Windows 7, high-level platform APIs that use Core Audio APIs such as Media Foundation, DirectSound, and Wave APIs, implement the stream routing feature by handling stream switching from an existing device to a new default audio endpoint. Media applications that use these APIs (for example, an application activating an IDirectSound or IBaseFilter object on an IMMDevice object) use the stream routing behavior without any modifications to the source.

The high-level APIs implement stream routing for the device endpoint that is obtained through IMMDeviceEnumerator::GetDefaultAudioEndpoint. If an application streams to the default device, the stream routing feature operates as defined. Streams are not switched to the new device if it is retrieved by any other mechanism even if it is the same as the default device.

A media application that uses Core Audio APIs directly (WASAPI client) can provide a custom stream routing implementation for any rendering or capture device. A WASAPI client can replicate the implementation provided by the high-level APIs by restricting it to streams that are opened on devices that are set as the default device. To get a reference to the default device's endpoint, the client must call IMMDeviceEnumerator::GetDefaultAudioEndpoint. In this call, the client must indicate whether it requires a pointer to the rendering default device or the capture default device by specifying the dataFlow parameter. The client must also specify the appropriate role for the endpoint in the ERole attribute (eConsole or eCommunications). Do not use eMultimedia.

If the application streams to any other device, the application can get the device by specifying an endpoint ID string (by calling IMMDeviceEnumerator::GetDevice).

After the device is identified, the WASAPI client can provide the implementation for stream routing by handling the device and audio session notifications sent for the device. For more information about these notifications, see Relevant Notifications for Stream Routing.
Getting the Device Endpoint for Stream Routing
06/09/2025
In Windows 7, high-level platform APIs that use Core Audio APIs such as Media Foundation, DirectSound, and Wave APIs, implement the stream routing feature by handling stream switching from an existing device to a new default audio endpoint. Media applications that use these APIs (for example, an application activating an IDirectSound or IBaseFilter object on an IMMDevice object) use the stream routing behavior without any modifications to the source.

The high-level APIs implement stream routing for the device endpoint that is obtained through IMMDeviceEnumerator::GetDefaultAudioEndpoint. If an application streams to the default device, the stream routing feature operates as defined. Streams are not switched to the new device if it is retrieved by any other mechanism even if it is the same as the default device.

A media application that uses Core Audio APIs directly (WASAPI client) can provide a custom stream routing implementation for any rendering or capture device. A WASAPI client can replicate the implementation provided by the high-level APIs by restricting it to streams that are opened on devices that are set as the default device. To get a reference to the default device's endpoint, the client must call IMMDeviceEnumerator::GetDefaultAudioEndpoint. In this call, the client must indicate whether it requires a pointer to the rendering default device or the capture default device by specifying the dataFlow parameter. The client must also specify the appropriate role for the endpoint in the ERole attribute (eConsole or eCommunications). Do not use eMultimedia.

If the application streams to any other device, the application can get the device by specifying an endpoint ID string (by calling IMMDeviceEnumerator::GetDevice).

After the device is identified, the WASAPI client can provide the implementation for stream routing by handling the device and audio session notifications sent for the device. For more information about these notifications, see Relevant Notifications for Stream Routing.
Relevant Notifications for Stream Routing
06/14/2021
In Windows 7, high-level platform APIs that use Core Audio APIs, such as Media Foundation, DirectSound, and Wave APIs, implement the stream routing feature by handling stream switching from an existing device to a new default audio endpoint. Media applications that use these APIs use the stream routing behavior without any modifications to the source. Direct WASAPI clients can use the notifications sent by Core Audio components and implement the stream routing feature.

To implement the stream routing feature, a client must listen for two types of events: device change notifications and session disconnect notifications. In the implementation provided by the high-level APIs, these events are sent for default device endpoints created by calling IMMDeviceEnumerator::GetDefaultAudioEndpoint. For more information, see Getting the Device Endpoint for Stream Routing.

The Core Audio component MMDeviceAPI delivers notification callbacks when audio devices are added, removed, or modified. The format and audio session changes are reported as events through WASAPI.

Device Change Notifications
MMDeviceAPI raises events when audio devices are added, removed, or modified. If the client is to provide stream routing functionality, it must implement the IMMNotificationClient interface and register its implementation with MMDeviceAPI.

To get device change notifications, the client must perform the following tasks:

Implement the IMMNotificationClient interface to handle device change notifications sent by MMDeviceAPI.
Register the IMMNotificationClient implementation with MMDeviceAPI by calling the IMMDeviceEnumerator::RegisterEndpointNotificationCallback method.
After the client's implementation of these interfaces is registered, the client receives notifications in the form of callbacks through the methods of these interfaces. IMMNotificationClient methods are called by MMDeviceAPI when it raises endpoint-level events (endpoint state changes, new endpoint arrivals, endpoint deletions, default endpoint changes, and endpoint property changes).

If the client wants to provide stream routing for the default device, the client must implement the device change behavior when it receives the notification through the IMMNotificationClient::OnDefaultDeviceChanged callback.

Audio Session Change Notifications
Audio session changes and format changes are reported as audio session events through WASAPI. A WASAPI client implements the IAudioSessionEvents interface and registers the implementation with WASAPI.

To get audio session change notifications, the client must perform the following tasks:

Implement the IAudioSessionEvents interface to handle format change notifications sent by WASAPI.
Register the IAudioSessionEvents implementation with WASAPI by calling the IAudioSessionControl::RegisterAudioSessionNotification method.
IAudioSessionEvents methods are called by WASAPI when audio session changes occur. These events are raised when the session's display name, icon path, volume, grouping parameter, or state changes.

To implement the stream routing feature, the client must wait for the session-disconnect notification. When an audio session is disconnected or a device's format is changed, WASAPI sends the client notifications in the form of callbacks through IAudioSessionEvents::OnSessionDisconnected. With the disconnection notification, WASAPI also sends a value that indicates why the session was disconnected. This can occur for several reasons, such as the device was removed, the server stopped, and so on. For the complete list of reasons, see the AudioSessionDisconnectReason enumeration defined in AudioPolicy.h. If the default device changes, the client must wait for the notifications (if they have not already been received) that are accompanied with a DisconnectReasonDeviceRemoval value. In response to such notifications, the client might reopen the stream on the new default device.

Because all of these operations are asychronous, the order in which the application receives notifications cannnot be predicted. However, typically, the application receives AudioSessionDisconnect value before the default device change notification.

Format Change Notifications
Audio format changes occur when the format of the stream changes. This can occur when the user selects a new format in the Sound control panel or the new default device supports a new format (for example, HDMI or certain professional audio interfaces with a manual sample-rate adjustment). To notify the client about these types of format changes, WASAPI sends a session notification by the registered implementation of IAudioSessionEvents::OnSessionDisconnected with a disconnect reason of DisconnectReasonFormatChanged. The client can handle the notification by reopening the stream in the new format.

Stream Routing Implementation Considerations
06/14/2021
In Windows 7, high-level platform APIs that use Core Audio APIs, such as Media Foundation, DirectSound, and Wave APIs, implement the stream routing feature by handling stream switching from an existing device to a new default audio endpoint. Media applications that use these APIs use the stream routing behavior without any modifications to the source. Direct WASAPI clients can use the notifications sent by Core Audio components and implement the stream routing feature.

Direct WASAPI clients (media applications that use WASAPI directly) receive new device and audio session notifications sent by Core Audio components. The behavior of the stream routing feature is defined by how the application handles these notifications.

MMDevice API and the audio session send notifications about device state changes and session changes to WASAPI clients in the form of callbacks. To get these notifications, the client must register its implementation of IMMNotificationClient and IAudioSessionEvents. For more information, see Relevant Notifications for Stream Routing.

In the USB headset scenario described in Stream Routing, an application is playing an audio stream and uses MMDeviceAPI and WASAPI to render the stream on the default rendering device, Speaker. When the default device is changed, the application receives an IMMNotificationClient notification. The application also receives IAudioSessionEvents notifications indicating that the user removed the audio endpoint device or that the stream format changed for the device that the audio session is connected to. Upon receiving the notifications, the application stops streaming to the speaker endpoint and reopens the stream for rendering on the current default endpoint, the headset.

diagram of data flow for device notifications.

In response to such notifications, the client might reopen the stream on the new default device in the new format selected by the user.

Stream Managment
The following list summarizes the steps that a WASAPI client must perform to provide the stream switching functionality.

Wait for the relevant IMMNotificationClient notification. If the device is the default device, the IMMNotificationClient::OnDefaultDeviceChanged notification is received.

If a new device is available, get a reference to the endpoint of the new device. Call IMMDeviceEnumerator::GetDefaultAudioEndpoint for the new default device. If the new device is not the default device, you can retrieve the device by calling IMMDeviceEnumerator::GetDevice. For more information, see Getting the Device Endpoint for Stream Routing.

Wait for the IAudioSessionEvents::OnSessionDisconnected with the reason value.

 Note

Because all of these operations are asychronous, the order in which the application receives device-change and session-disconnect notifications cannnot be predicted. The application must implement notification handling to receive these notifications in any order. However, typically, the application receives AudioSessionDisconnect value before the default device change notification.

 

Evaluate the reason value and determine whether the stream needs to be transferred to another audio endpoint or the stream needs to be reinitialized with a new format.

Stop streaming to the old default device if the reason indicates that the stream should be re-routed to the new default device.

Perform position mapping calculations.

Open the stream on the new device and transfer all state information.

Resume streaming on the new default device.

Handle the departure of the old default device.

To make the stream switching operation appear seamless, it must be done as quickly as possible. This depends on the performance of the components involved in re-initiation of the stream on the new device.

Position Mapping Considerations
When the application gets IMMNotificationClient and IAudioSessionEvents notifications, it can route the existing streams to the new default device. When an existing audio stream is interrupted and opened on the new device, rendering on the new device must start at the position at which the stream was stopped on the old device. To do this, the application must have the last known device position, to calculate the start position on the new device. For example, this position can be used as the delta offset for subsequent position mapping. When the stream starts rendering, the new device position can be remapped to the cached device position.

The following steps summarize the process of making a seamless stream transition.

Cache the last device position of the stream on the old device.
Stop the stream on the old device.
Perform remapping calculations to get the new position.
Start rendering the stream on the new device.
Release the old stream.
During the transition, the application must ensure that the clock does not get out of synchronization, resulting in out-of-sync audio and video streams. This can occur if the video samples continue to render while the audio stream is routed to the new device. The application must cache the clock position for the remapping calculation and make sure that the video samples are not rendered until the audio stream is reopened on the new device, so that when the clip resumes rendering, the audio and the video streams are synchronized. In some cases, where the presentation time for rendering the video frames is based on the audio clock, it is sufficient to stop the audio stream until stream switching is complete and no other position mapping implementation for the video stream is necessary for audio video synchronization.

If while rendering, IAudioRenderClient::GetBuffer returns an error because the old device is lost, the application need not stop the old stream because the streaming operation has already terminated. For information about handling this error, see Recovering from an Invalid-Device Error.

ender Spatial Sound Using Spatial Audio Objects
01/06/2021
This article presents some simple examples that illustrate how to implement spatial sound using static spatial audio objects, dynamic spatial audio objects, and spatial audio objects that use Microsoft's Head Relative Transfer Function (HRTF). The implementation steps for all three of these techniques are very similar and this article provides a similarly structured code example for each technique. For complete end-to-end examples of real-world spatial audio implementations, see Microsoft Spatial Sound samples github repository. For an overview of Windows Sonic, Microsoft’s platform-level solution for spatial sound support on Xbox and Windows, see Spatial Sound.

Render audio using static spatial audio objects
A static audio object is used to render sound to one of 18 static audio channels defined in the AudioObjectType enumeration. Each of these channels represents a real or virtualized speaker at a fixed point in space that does not move over time. The static channels that are available on a particular device depend on the spatial sound format being used. For a list of the supported formats and their channel counts, see Spatial Sound.

When you initialize a spatial audio stream, you must specify which of the available static channels the stream will use. The following example constant definitions can be used to specify common speaker configurations and get the number of channels available for each one.

C++

Copy
const AudioObjectType ChannelMask_Mono = AudioObjectType_FrontCenter;
const AudioObjectType ChannelMask_Stereo = (AudioObjectType)(AudioObjectType_FrontLeft | AudioObjectType_FrontRight);
const AudioObjectType ChannelMask_2_1 = (AudioObjectType)(ChannelMask_Stereo | AudioObjectType_LowFrequency);
const AudioObjectType ChannelMask_Quad = (AudioObjectType)(AudioObjectType_FrontLeft | AudioObjectType_FrontRight | AudioObjectType_BackLeft | AudioObjectType_BackRight);
const AudioObjectType ChannelMask_4_1 = (AudioObjectType)(ChannelMask_Quad | AudioObjectType_LowFrequency);
const AudioObjectType ChannelMask_5_1 = (AudioObjectType)(AudioObjectType_FrontLeft | AudioObjectType_FrontRight | AudioObjectType_FrontCenter | AudioObjectType_LowFrequency | AudioObjectType_SideLeft | AudioObjectType_SideRight);
const AudioObjectType ChannelMask_7_1 = (AudioObjectType)(ChannelMask_5_1 | AudioObjectType_BackLeft | AudioObjectType_BackRight);

const UINT32 MaxStaticObjectCount_7_1_4 = 12;
const AudioObjectType ChannelMask_7_1_4 = (AudioObjectType)(ChannelMask_7_1 | AudioObjectType_TopFrontLeft | AudioObjectType_TopFrontRight | AudioObjectType_TopBackLeft | AudioObjectType_TopBackRight);

const UINT32 MaxStaticObjectCount_7_1_4_4 = 16;
const AudioObjectType ChannelMask_7_1_4_4 = (AudioObjectType)(ChannelMask_7_1_4 | AudioObjectType_BottomFrontLeft | AudioObjectType_BottomFrontRight |AudioObjectType_BottomBackLeft | AudioObjectType_BottomBackRight);

const UINT32 MaxStaticObjectCount_8_1_4_4 = 17;
const AudioObjectType ChannelMask_8_1_4_4 = (AudioObjectType)(ChannelMask_7_1_4_4 | AudioObjectType_BackCenter);
The first step in rendering spatial audio is to get the audio endpoint to which audio data will be sent. Create an instance of MMDeviceEnumerator and call GetDefaultAudioEndpoint to get the default audio render device.

C++

Copy
HRESULT hr;
Microsoft::WRL::ComPtr<IMMDeviceEnumerator> deviceEnum;
Microsoft::WRL::ComPtr<IMMDevice> defaultDevice;

hr = CoCreateInstance(__uuidof(MMDeviceEnumerator), nullptr, CLSCTX_ALL, __uuidof(IMMDeviceEnumerator), (void**)&deviceEnum);
hr = deviceEnum->GetDefaultAudioEndpoint(EDataFlow::eRender, eMultimedia, &defaultDevice);
When you create a spatial audio stream, you must specify the audio format the stream will use by providing a WAVEFORMATEX structure. If you are playing back audio from a file, the format is typically determined by the audio file format. This example uses a mono, 32-bit, 48Hz format.

C++

Copy
WAVEFORMATEX format;
format.wFormatTag = WAVE_FORMAT_IEEE_FLOAT;
format.wBitsPerSample = 32;
format.nChannels = 1;
format.nSamplesPerSec = 48000;
format.nBlockAlign = (format.wBitsPerSample >> 3) * format.nChannels;
format.nAvgBytesPerSec = format.nBlockAlign * format.nSamplesPerSec;
format.cbSize = 0;
The next step in rendering spatial audio is to initialize a spatial audio stream. First, get an instance of ISpatialAudioClient by calling IMMDevice::Activate. Call ISpatialAudioClient::IsAudioObjectFormatSupported to make sure that the audio format you are using is supported. Create an event that the audio pipeline will use to notify the app that it is ready for more audio data.

Populate a SpatialAudioObjectRenderStreamActivationParams structure that will be used to initialize the spatial audio stream. In this example, the StaticObjectTypeMask field is set to the ChannelMask_Stereo constant defined previously in this article, meaning that only the front right and left channels can be used by the audio stream. Because this example uses only static audio objects and no dynamic objects, the MaxDynamicObjectCount field is set to 0. The Category field is set to a member of the AUDIO_STREAM_CATEGORY enumeration, which defines how the system mixes the sound from this stream with other audio sources.

Call ISpatialAudioClient::ActivateSpatialAudioStream to activate the stream.

C++

Copy
Microsoft::WRL::ComPtr<ISpatialAudioClient> spatialAudioClient;

// Activate ISpatialAudioClient on the desired audio-device 
hr = defaultDevice->Activate(__uuidof(ISpatialAudioClient), CLSCTX_INPROC_SERVER, nullptr, (void**)&spatialAudioClient);

hr = spatialAudioClient->IsAudioObjectFormatSupported(&format);

// Create the event that will be used to signal the client for more data
HANDLE bufferCompletionEvent = CreateEvent(nullptr, FALSE, FALSE, nullptr);

SpatialAudioObjectRenderStreamActivationParams streamParams;
streamParams.ObjectFormat = &format;
streamParams.StaticObjectTypeMask = ChannelMask_Stereo;
streamParams.MinDynamicObjectCount = 0;
streamParams.MaxDynamicObjectCount = 0;
streamParams.Category = AudioCategory_SoundEffects;
streamParams.EventHandle = bufferCompletionEvent;
streamParams.NotifyObject = nullptr;

PROPVARIANT activationParams;
PropVariantInit(&activationParams);
activationParams.vt = VT_BLOB;
activationParams.blob.cbSize = sizeof(streamParams);
activationParams.blob.pBlobData = reinterpret_cast<BYTE *>(&streamParams);

Microsoft::WRL::ComPtr<ISpatialAudioObjectRenderStream> spatialAudioStream;
hr = spatialAudioClient->ActivateSpatialAudioStream(&activationParams, __uuidof(spatialAudioStream), (void**)&spatialAudioStream);
 Note

When using the ISpatialAudioClient interfaces on an Xbox One Development Kit (XDK) title, you must first call EnableSpatialAudio before calling IMMDeviceEnumerator::EnumAudioEndpoints or IMMDeviceEnumerator::GetDefaultAudioEndpoint. Failure to do so will result in an E_NOINTERFACE error being returned from the call to Activate. EnableSpatialAudio is only available for XDK titles, and does not need to be called for Universal Windows Platform apps running on Xbox One, nor for any non-Xbox One devices.

 

Declare a pointer for an ISpatialAudioObject that will be used to write audio data to a static channel. Typical apps will use an object for each channel specified in the StaticObjectTypeMask field. For simplicity, this example only uses a single static audio object.

C++

Copy
// In this simple example, one object will be rendered
Microsoft::WRL::ComPtr<ISpatialAudioObject> audioObjectFrontLeft;
Before entering the audio render loop, call ISpatialAudioObjectRenderStream::Start to instruct the media pipeline to begin requesting audio data. This example uses a counter to stop the rendering of audio after 5 seconds.

Inside the render loop, wait for the buffer completion event, provided when the spatial audio stream was initialized, to be signaled. You should set a reasonable timeout limit, like 100 ms, when waiting for the event because any change to the render type or endpoint will cause that event to never be signaled. In this case, you can call ISpatialAudioObjectRenderStream::Reset to attempt to reset the spatial audio stream.

Next, call ISpatialAudioObjectRenderStream::BeginUpdatingAudioObjects to let the system know that you are about to fill the audio objects' buffers with data. This method returns the number of available dynamic audio objects, not used in this example, and the frame count of the buffer for audio objects rendered by this stream.

If a static spatial audio object has not yet been created, create one or more by calling ISpatialAudioObjectRenderStream::ActivateSpatialAudioObject, passing in a value from the AudioObjectType enumeration indicating the static channel to which the object's audio is rendered.

Next, call ISpatialAudioObject::GetBuffer to get a pointer to the spatial audio object's audio buffer. This method also returns the size of the buffer, in bytes. This example uses a helper method, WriteToAudioObjectBuffer, to fill the buffer with audio data. This method is shown later in this article. After writing to the buffer, the example checks to see if the 5 second lifetime of the object has been reached, and if so, ISpatialAudioObject::SetEndOfStream is called to let the audio pipeline know that no more audio will be written using this object and the object is set to nullptr to free up its resources.

After writing data to all of your audio objects, call ISpatialAudioObjectRenderStream::EndUpdatingAudioObjects to let the system know the data is ready for rendering. You can only call GetBuffer in between a call to BeginUpdatingAudioObjects and EndUpdatingAudioObjects.

C++

Copy
// Start streaming / rendering 
hr = spatialAudioStream->Start();

// This example will render 5 seconds of audio samples
UINT totalFrameCount = format.nSamplesPerSec * 5;

bool isRendering = true;
while (isRendering)
{
    // Wait for a signal from the audio-engine to start the next processing pass
    if (WaitForSingleObject(bufferCompletionEvent, 100) != WAIT_OBJECT_0)
    {
        hr = spatialAudioStream->Reset();

        if (hr != S_OK)
        {
            // handle the error
            break;
        }
    }

    UINT32 availableDynamicObjectCount;
    UINT32 frameCount;

    // Begin the process of sending object data and metadata
    // Get the number of dynamic objects that can be used to send object-data
    // Get the frame count that each buffer will be filled with 
    hr = spatialAudioStream->BeginUpdatingAudioObjects(&availableDynamicObjectCount, &frameCount);

    BYTE* buffer;
    UINT32 bufferLength;

    if (audioObjectFrontLeft == nullptr)
    {
        hr = spatialAudioStream->ActivateSpatialAudioObject(AudioObjectType::AudioObjectType_FrontLeft, &audioObjectFrontLeft);
        if (hr != S_OK) break;
    }

    // Get the buffer to write audio data
    hr = audioObjectFrontLeft->GetBuffer(&buffer, &bufferLength);

    if (totalFrameCount >= frameCount)
    {
        // Write audio data to the buffer
        WriteToAudioObjectBuffer(reinterpret_cast<float*>(buffer), frameCount, 200.0f, format.nSamplesPerSec);

        totalFrameCount -= frameCount;
    }
    else
    {
        // Write audio data to the buffer
        WriteToAudioObjectBuffer(reinterpret_cast<float*>(buffer), totalFrameCount, 750.0f, format.nSamplesPerSec);

        // Set end of stream for the last buffer 
        hr = audioObjectFrontLeft->SetEndOfStream(totalFrameCount);

        audioObjectFrontLeft = nullptr; // Release the object

        isRendering = false;
    }

    // Let the audio engine know that the object data are available for processing now
    hr = spatialAudioStream->EndUpdatingAudioObjects();
};
When you are done rendering spatial audio, stop the spatial audio stream by calling ISpatialAudioObjectRenderStream::Stop. If you are not going to use the stream again, free its resources by calling ISpatialAudioObjectRenderStream::Reset.

C++

Copy
// Stop the stream
hr = spatialAudioStream->Stop();

// Don't want to start again, so reset the stream to free its resources
hr = spatialAudioStream->Reset();

CloseHandle(bufferCompletionEvent);
The WriteToAudioObjectBuffer helper method writes either a full buffer of samples or the number of remaining samples specified by our app-defined time limit. This could also be determined, for example, by the number of samples remaining in a source audio file. A simple sin wave, the frequency of which is scaled by the frequency input parameter, is generated and written to the buffer.

C++

Copy
void WriteToAudioObjectBuffer(FLOAT* buffer, UINT frameCount, FLOAT frequency, UINT samplingRate)
{
    const double PI = 4 * atan2(1.0, 1.0);
    static double _radPhase = 0.0;

    double step = 2 * PI * frequency / samplingRate;

    for (UINT i = 0; i < frameCount; i++)
    {
        double sample = sin(_radPhase);

        buffer[i] = FLOAT(sample);

        _radPhase += step; // next frame phase

        if (_radPhase >= 2 * PI)
        {
            _radPhase -= 2 * PI;
        }
    }
}
Render audio using dynamic spatial audio objects
Dynamic objects allow you to render audio from an arbitrary position in space, relative to the user. The position and volume of a dynamic audio object can be changed over time. Games will typically use the position of a 3D object in the game world to specify the position of the dynamic audio object associated with it. The following example will use a simple structure, My3dObject, to store the minimum set of data needed to represent an object. This data includes a pointer to an ISpatialAudioObject, the position, velocity, volume, and tone frequency for the object, and a value that stores the total number of frames for which the object should render sound.

C++

Copy
struct My3dObject
{
    Microsoft::WRL::ComPtr<ISpatialAudioObject> audioObject;
    Windows::Foundation::Numerics::float3 position;
    Windows::Foundation::Numerics::float3 velocity;
    float volume;
    float frequency; // in Hz
    UINT totalFrameCount;
};
The implementation steps for dynamic audio objects is largely the same as the steps for static audio objects described above. First, get an audio endpoint.

C++

Copy
HRESULT hr;
Microsoft::WRL::ComPtr<IMMDeviceEnumerator> deviceEnum;
Microsoft::WRL::ComPtr<IMMDevice> defaultDevice;

hr = CoCreateInstance(__uuidof(MMDeviceEnumerator), nullptr, CLSCTX_ALL, __uuidof(IMMDeviceEnumerator), (void**)&deviceEnum);
hr = deviceEnum->GetDefaultAudioEndpoint(EDataFlow::eRender, eMultimedia, &defaultDevice);
Next, initialize the spatial audio stream. Get an instance of ISpatialAudioClient by calling IMMDevice::Activate. Call ISpatialAudioClient::IsAudioObjectFormatSupported to make sure that the audio format you are using is supported. Create an event that the audio pipeline will use to notify the app that it is ready for more audio data.

Call ISpatialAudioClient::GetMaxDynamicObjectCount to retrieve the number of dynamic objects supported by the system. If this call returns 0, then dynamic spatial audio objects are not supported or enabled on the current device. For information on enabling spatial audio and for details on the number of dynamic audio objects available for different spatial audio formats, see Spatial Sound.

When populating the SpatialAudioObjectRenderStreamActivationParams structure, set the MaxDynamicObjectCount field to the maximum number of dynamic objects your app will use.

Call ISpatialAudioClient::ActivateSpatialAudioStream to activate the stream.

C++

Copy
// Activate ISpatialAudioClient on the desired audio-device 
Microsoft::WRL::ComPtr<ISpatialAudioClient> spatialAudioClient;
hr = defaultDevice->Activate(__uuidof(ISpatialAudioClient), CLSCTX_INPROC_SERVER, nullptr, (void**)&spatialAudioClient);

hr = spatialAudioClient->IsAudioObjectFormatSupported(&format);

// Create the event that will be used to signal the client for more data
HANDLE bufferCompletionEvent = CreateEvent(nullptr, FALSE, FALSE, nullptr);

UINT32 maxDynamicObjectCount;
hr = spatialAudioClient->GetMaxDynamicObjectCount(&maxDynamicObjectCount);

if (maxDynamicObjectCount == 0)
{
    // Dynamic objects are unsupported
    return;
}

// Set the maximum number of dynamic audio objects that will be used
SpatialAudioObjectRenderStreamActivationParams streamParams;
streamParams.ObjectFormat = &format;
streamParams.StaticObjectTypeMask = AudioObjectType_None;
streamParams.MinDynamicObjectCount = 0;
streamParams.MaxDynamicObjectCount = min(maxDynamicObjectCount, 4);
streamParams.Category = AudioCategory_GameEffects;
streamParams.EventHandle = bufferCompletionEvent;
streamParams.NotifyObject = nullptr;

PROPVARIANT pv;
PropVariantInit(&pv);
pv.vt = VT_BLOB;
pv.blob.cbSize = sizeof(streamParams);
pv.blob.pBlobData = (BYTE *)&streamParams;

Microsoft::WRL::ComPtr<ISpatialAudioObjectRenderStream> spatialAudioStream;
hr = spatialAudioClient->ActivateSpatialAudioStream(&pv, __uuidof(spatialAudioStream), (void**)&spatialAudioStream);
The following is some app-specific code to needed support this example, which will dynamically spawn randomly positioned audio objects and store them in a vector.

C++

Copy
// Used for generating a vector of randomized My3DObject structs
std::vector<My3dObject> objectVector;
std::default_random_engine gen;
std::uniform_real_distribution<> pos_dist(-25, 25); // uniform distribution for random position
std::uniform_real_distribution<> vel_dist(-1, 1); // uniform distribution for random velocity
std::uniform_real_distribution<> vol_dist(0.5, 1.0); // uniform distribution for random volume
std::uniform_real_distribution<> pitch_dist(40, 400); // uniform distribution for random pitch
int spawnCounter = 0;
Before entering the audio render loop, call ISpatialAudioObjectRenderStream::Start to instruct the media pipeline to begin requesting audio data.

Inside the render loop, wait for the buffer completion event we provided when the spatial audio stream was initialized to be signaled. You should set a reasonable timeout limit, like 100 ms, when waiting for the event because any change to the render type or endpoint will cause that event to never be signaled. In this case, you can call ISpatialAudioObjectRenderStream::Reset to attempt to reset the spatial audio stream.

Next, call ISpatialAudioObjectRenderStream::BeginUpdatingAudioObjects to let the system know that you are about to fill the audio objects' buffers with data. This method returns the number of available dynamic audio objects and the frame count of the buffer for audio objects rendered by this stream.

Whenever the spawn counter reaches the specified value, we will activate a new dynamic audio object by calling ISpatialAudioObjectRenderStream::ActivateSpatialAudioObject specifying AudioObjectType_Dynamic. If all available dynamic audio objects have already been allocated, this method will return SPLAUDCLNT_E_NO_MORE_OBJECTS. In this case, you can choose to release one or more previously activated audio objects based on your app-specific prioritization. After the dynamic audio object has been created, it is added to a new My3dObject structure, with randomized position, velocity, volume, and frequency values, which is then added to the list of active objects.

Next, iterate over all of the active objects, represented in this example with the app-defined My3dObject structure. For each audio object, call ISpatialAudioObject::GetBuffer to get a pointer to the spatial audio object's audio buffer. This method also returns the size of the buffer, in bytes. The helper method, WriteToAudioObjectBuffer, to fill the buffer with audio data. After writing to the buffer, the example updates the position of the dynamic audio object by calling ISpatialAudioObject::SetPosition. The volume of the audio object can also be modified by calling SetVolume. If you don't update the position or volume of the object, it will retain the position and volume from the last time it was set. If the object's app-defined lifetime has been reached, ISpatialAudioObject::SetEndOfStream is called to let the audio pipeline know that no more audio will be written using this object and the object is set to nullptr to free up its resources.

After writing data to all of your audio objects, call ISpatialAudioObjectRenderStream::EndUpdatingAudioObjects to let the system know the data is ready for rendering. You can only call GetBuffer in between a call to BeginUpdatingAudioObjects and EndUpdatingAudioObjects.

C++

Copy
// Start streaming / rendering 
hr = spatialAudioStream->Start();

do
{
    // Wait for a signal from the audio-engine to start the next processing pass
    if (WaitForSingleObject(bufferCompletionEvent, 100) != WAIT_OBJECT_0)
    {
        break;
    }

    UINT32 availableDynamicObjectCount;
    UINT32 frameCount;

    // Begin the process of sending object data and metadata
    // Get the number of active objects that can be used to send object-data
    // Get the frame count that each buffer will be filled with 
    hr = spatialAudioStream->BeginUpdatingAudioObjects(&availableDynamicObjectCount, &frameCount);

    BYTE* buffer;
    UINT32 bufferLength;

    // Spawn a new dynamic audio object every 200 iterations
    if (spawnCounter % 200 == 0 && spawnCounter < 1000)
    {
        // Activate a new dynamic audio object
        Microsoft::WRL::ComPtr<ISpatialAudioObject> audioObject;
        hr = spatialAudioStream->ActivateSpatialAudioObject(AudioObjectType::AudioObjectType_Dynamic, &audioObject);

        // If SPTLAUDCLNT_E_NO_MORE_OBJECTS is returned, there are no more available objects
        if (SUCCEEDED(hr))
        {
            // Init new struct with the new audio object.
            My3dObject obj = {
                audioObject,
                Windows::Foundation::Numerics::float3(static_cast<float>(pos_dist(gen)), static_cast<float>(pos_dist(gen)), static_cast<float>(pos_dist(gen))),
                Windows::Foundation::Numerics::float3(static_cast<float>(vel_dist(gen)), static_cast<float>(vel_dist(gen)), static_cast<float>(vel_dist(gen))),
                static_cast<float>(static_cast<float>(vol_dist(gen))),
                static_cast<float>(static_cast<float>(pitch_dist(gen))),
                format.nSamplesPerSec * 5 // 5 seconds of audio samples
            };

            objectVector.insert(objectVector.begin(), obj);
        }
    }
    spawnCounter++;

    // Loop through all dynamic audio objects
    std::vector<My3dObject>::iterator it = objectVector.begin();
    while (it != objectVector.end())
    {
        it->audioObject->GetBuffer(&buffer, &bufferLength);

        if (it->totalFrameCount >= frameCount)
        {
            // Write audio data to the buffer
            WriteToAudioObjectBuffer(reinterpret_cast<float*>(buffer), frameCount, it->frequency, format.nSamplesPerSec);

            // Update the position and volume of the audio object
            it->audioObject->SetPosition(it->position.x, it->position.y, it->position.z);
            it->position += it->velocity;
            it->audioObject->SetVolume(it->volume);

            it->totalFrameCount -= frameCount;

            ++it;
        }
        else
        {
            // If the audio object reaches its lifetime, set EndOfStream and release the object

            // Write audio data to the buffer
            WriteToAudioObjectBuffer(reinterpret_cast<float*>(buffer), it->totalFrameCount, it->frequency, format.nSamplesPerSec);

            // Set end of stream for the last buffer 
            hr = it->audioObject->SetEndOfStream(it->totalFrameCount);

            it->audioObject = nullptr; // Release the object

            it->totalFrameCount = 0;

            it = objectVector.erase(it);
        }
    }

    // Let the audio-engine know that the object data are available for processing now
    hr = spatialAudioStream->EndUpdatingAudioObjects();
} while (objectVector.size() > 0);
When you are done rendering spatial audio, stop the spatial audio stream by calling ISpatialAudioObjectRenderStream::Stop. If you are not going to use the stream again, free its resources by calling ISpatialAudioObjectRenderStream::Reset.

C++

Copy
// Stop the stream 
hr = spatialAudioStream->Stop();

// We don't want to start again, so reset the stream to free it's resources.
hr = spatialAudioStream->Reset();

CloseHandle(bufferCompletionEvent);
Render audio using dynamic spatial audio objects for HRTF
Another set of APIs, ISpatialAudioRenderStreamForHrtf and ISpatialAudioObjectForHrtf, enable spatial audio that uses Microsoft's Head-relative Transfer Function (HRTF) to attenuate sounds to simulate the emitter's position in space, relative to the user, which can be changed over time. In addition to position, HRTF audio objects allow you to specify an orientation in space, a directivity in which sound is emitted, such as a cone or cardioid shape, and a decay model as the object moves nearer and further from the virtual listener. Note that these HRTF interfaces are only available when the user has selected Windows Sonic for Headphones as the spatial audio engine for the device. For information on configuring a device to use Windows Sonic for Headphones, see Spatial Sound.

The ISpatialAudioRenderStreamForHrtf and ISpatialAudioObjectForHrtf APIs allow an application to explicitly use the Windows Sonic for Headphones render path directly. These APIs do not support spatial sound formats such as Dolby Atmos for Home Theater or Dolby Atmos for Headphones, nor consumer-controlled output format switching via the Sound control panel, nor playback over speakers. These interfaces are intended for use in Windows Mixed Reality applications that want to use Windows Sonic for Headphones-specific capabilities (such as environmental presets and distance-based rolloff specified programmatically, outside of typical content authoring pipelines). Most games and virtual reality scenarios will prefer to use ISpatialAudioClient instead. The implementation steps for both API sets are almost identical, so it is possible to implement both technologies and switch at runtime depending on which feature is available on the current device.

Mixed-reality apps will typically use the position of a 3D object in the virtual world to specify the position of the dynamic audio object associated with it. The following example will use a simple structure, My3dObjectForHrtf, to store the minimum set of data needed to represent an object. This data includes a pointer to an ISpatialAudioObjectForHrtf, the position, orientation, velocity, and tone frequency for the object, and a value that stores the total number of frames for which the object should render sound.

C++

Copy
struct My3dObjectForHrtf
{
    Microsoft::WRL::ComPtr<ISpatialAudioObjectForHrtf> audioObject;
    Windows::Foundation::Numerics::float3 position;
    Windows::Foundation::Numerics::float3 velocity;
    float yRotationRads;
    float deltaYRotation;
    float frequency; // in Hz
    UINT totalFrameCount;
};
The implementation steps for dynamic HRTF audio objects is largely the same as the steps for dynamic audio objects described in the previous section. First, get an audio endpoint.

C++

Copy
HRESULT hr;
Microsoft::WRL::ComPtr<IMMDeviceEnumerator> deviceEnum;
Microsoft::WRL::ComPtr<IMMDevice> defaultDevice;

hr = CoCreateInstance(__uuidof(MMDeviceEnumerator), nullptr, CLSCTX_ALL, __uuidof(IMMDeviceEnumerator), (void**)&deviceEnum);
hr = deviceEnum->GetDefaultAudioEndpoint(EDataFlow::eRender, eMultimedia, &defaultDevice);
Next, initialize the spatial audio stream. Get an instance of ISpatialAudioClient by calling IMMDevice::Activate. Call ISpatialAudioClient::IsAudioObjectFormatSupported to make sure that the audio format you are using is supported. Create an event that the audio pipeline will use to notify the app that it is ready for more audio data.

Call ISpatialAudioClient::GetMaxDynamicObjectCount to retrieve the number of dynamic objects supported by the system. If this call returns 0, then dynamic spatial audio objects are not supported or enabled on the current device. For information on enabling spatial audio and for details on the number of dynamic audio objects available for different spatial audio formats, see Spatial Sound.

When populating the SpatialAudioHrtfActivationParams structure, set the MaxDynamicObjectCount field to the maximum number of dynamic objects your app will use. The activation params for HRTF supports a few additional parameters, such as a SpatialAudioHrtfDistanceDecay, a SpatialAudioHrtfDirectivityUnion, a SpatialAudioHrtfEnvironmentType, and a SpatialAudioHrtfOrientation, which specify the default values of these settings for new objects created from the stream. These parameters are optional. Set the fields to nullptr to provide no default values.

Call ISpatialAudioClient::ActivateSpatialAudioStream to activate the stream.

C++

Copy
// Activate ISpatialAudioClient on the desired audio-device 
Microsoft::WRL::ComPtr<ISpatialAudioClient> spatialAudioClient;
hr = defaultDevice->Activate(__uuidof(ISpatialAudioClient), CLSCTX_INPROC_SERVER, nullptr, (void**)&spatialAudioClient);

Microsoft::WRL::ComPtr<ISpatialAudioObjectRenderStreamForHrtf>  spatialAudioStreamForHrtf;
hr = spatialAudioClient->IsSpatialAudioStreamAvailable(__uuidof(spatialAudioStreamForHrtf), NULL);

hr = spatialAudioClient->IsAudioObjectFormatSupported(&format);

// Create the event that will be used to signal the client for more data
HANDLE bufferCompletionEvent = CreateEvent(nullptr, FALSE, FALSE, nullptr);

UINT32 maxDynamicObjectCount;
hr = spatialAudioClient->GetMaxDynamicObjectCount(&maxDynamicObjectCount);

SpatialAudioHrtfActivationParams streamParams;
streamParams.ObjectFormat = &format;
streamParams.StaticObjectTypeMask = AudioObjectType_None;
streamParams.MinDynamicObjectCount = 0;
streamParams.MaxDynamicObjectCount = min(maxDynamicObjectCount, 4);
streamParams.Category = AudioCategory_GameEffects;
streamParams.EventHandle = bufferCompletionEvent;
streamParams.NotifyObject = NULL;

SpatialAudioHrtfDistanceDecay decayModel;
decayModel.CutoffDistance = 100;
decayModel.MaxGain = 3.98f;
decayModel.MinGain = float(1.58439 * pow(10, -5));
decayModel.Type = SpatialAudioHrtfDistanceDecayType::SpatialAudioHrtfDistanceDecay_NaturalDecay;
decayModel.UnityGainDistance = 1;

streamParams.DistanceDecay = &decayModel;

SpatialAudioHrtfDirectivity directivity;
directivity.Type = SpatialAudioHrtfDirectivityType::SpatialAudioHrtfDirectivity_Cone;
directivity.Scaling = 1.0f;

SpatialAudioHrtfDirectivityCone cone;
cone.directivity = directivity;
cone.InnerAngle = 0.1f;
cone.OuterAngle = 0.2f;

SpatialAudioHrtfDirectivityUnion directivityUnion;
directivityUnion.Cone = cone;
streamParams.Directivity = &directivityUnion;

SpatialAudioHrtfEnvironmentType environment = SpatialAudioHrtfEnvironmentType::SpatialAudioHrtfEnvironment_Large;
streamParams.Environment = &environment;

SpatialAudioHrtfOrientation orientation = { 1,0,0,0,1,0,0,0,1 }; // identity matrix
streamParams.Orientation = &orientation;

PROPVARIANT pv;
PropVariantInit(&pv);
pv.vt = VT_BLOB;
pv.blob.cbSize = sizeof(streamParams);
pv.blob.pBlobData = (BYTE *)&streamParams;

hr = spatialAudioClient->ActivateSpatialAudioStream(&pv, __uuidof(spatialAudioStreamForHrtf), (void**)&spatialAudioStreamForHrtf);
The following is some app-specific code to needed support this example, which will dynamically spawn randomly positioned audio objects and store them in a vector.

C++

Copy
// Used for generating a vector of randomized My3DObject structs
std::vector<My3dObjectForHrtf> objectVector;
std::default_random_engine gen;
std::uniform_real_distribution<> pos_dist(-10, 10); // uniform distribution for random position
std::uniform_real_distribution<> vel_dist(-.02, .02); // uniform distribution for random velocity
std::uniform_real_distribution<> yRotation_dist(-3.14, 3.14); // uniform distribution for y-axis rotation
std::uniform_real_distribution<> deltaYRotation_dist(.01, .02); // uniform distribution for y-axis rotation
std::uniform_real_distribution<> pitch_dist(40, 400); // uniform distribution for random pitch

int spawnCounter = 0;
Before entering the audio render loop, call ISpatialAudioObjectRenderStreamForHrtf::Start to instruct the media pipeline to begin requesting audio data.

Inside the render loop, wait for the buffer completion event we provided when the spatial audio stream was initialized to be signaled. You should set a reasonable timeout limit, like 100 ms, when waiting for the event because any change to the render type or endpoint will cause that event to never be signaled. In this case, you can call ISpatialAudioRenderStreamForHrtf::Reset to attempt to reset the spatial audio stream.

Next, call ISpatialAudioRenderStreamForHrtf::BeginUpdatingAudioObjects to let the system know that you are about to fill the audio objects' buffers with data. This method returns the number of available dynamic audio objects, not used in this example, and the frame count of the buffer for audio objects rendered by this stream.

Whenever the spawn counter reaches the specified value, we will activate a new dynamic audio object by calling ISpatialAudioRenderStreamForHrtf::ActivateSpatialAudioObjectForHrtf specifying AudioObjectType_Dynamic. If all available dynamic audio objects have already been allocated, this method will return SPLAUDCLNT_E_NO_MORE_OBJECTS. In this case, you can choose to release one or more previously activated audio objects based on your app-specific prioritization. After the dynamic audio object has been created, it is added to a new My3dObjectForHrtf structure, with randomized position, rotation, velocity, volume, and frequency values, which is then added to the list of active objects.

Next, iterate over all of the active objects, represented in this example with the app-defined My3dObjectForHrtf structure. For each audio object, call ISpatialAudioObjectForHrtf::GetBuffer to get a pointer to the spatial audio object's audio buffer. This method also returns the size of the buffer, in bytes. The helper method, WriteToAudioObjectBuffer, listed previously in this article, to fill the buffer with audio data. After writing to the buffer, the example updates the position and orientation of the HRTF audio object by calling ISpatialAudioObjectForHrtf::SetPosition and ISpatialAudioObjectForHrtf::SetOrientation. In this example, a helper method, CalculateEmitterConeOrientationMatrix, is used to calculate the orientation matrix given the direction the 3D object is pointing. The implementation of this method is shown below. The volume of the audio object can also be modified by calling ISpatialAudioObjectForHrtf::SetGain. If you don't update the position, orientation, or volume of the object, it will retain the position, orientation, and volume from the last time it was set. If the object's app-defined lifetime has been reached, ISpatialAudioObjectForHrtf::SetEndOfStream is called to let the audio pipeline know that no more audio will be written using this object and the object is set to nullptr to free up its resources.

After writing data to all of your audio objects, call ISpatialAudioRenderStreamForHrtf::EndUpdatingAudioObjects to let the system know the data is ready for rendering. You can only call GetBuffer in between a call to BeginUpdatingAudioObjects and EndUpdatingAudioObjects.

C++

Copy
// Start streaming / rendering 
hr = spatialAudioStreamForHrtf->Start();

do
{
    // Wait for a signal from the audio-engine to start the next processing pass
    if (WaitForSingleObject(bufferCompletionEvent, 100) != WAIT_OBJECT_0)
    {
        break;
    }

    UINT32 availableDynamicObjectCount;
    UINT32 frameCount;

    // Begin the process of sending object data and metadata
    // Get the number of active objects that can be used to send object-data
    // Get the frame count that each buffer will be filled with 
    hr = spatialAudioStreamForHrtf->BeginUpdatingAudioObjects(&availableDynamicObjectCount, &frameCount);

    BYTE* buffer;
    UINT32 bufferLength;

    // Spawn a new dynamic audio object every 200 iterations
    if (spawnCounter % 200 == 0 && spawnCounter < 1000)
    {
        // Activate a new dynamic audio object
        Microsoft::WRL::ComPtr<ISpatialAudioObjectForHrtf> audioObject;
        hr = spatialAudioStreamForHrtf->ActivateSpatialAudioObjectForHrtf(AudioObjectType::AudioObjectType_Dynamic, &audioObject);

        // If SPTLAUDCLNT_E_NO_MORE_OBJECTS is returned, there are no more available objects
        if (SUCCEEDED(hr))
        {
            // Init new struct with the new audio object.
            My3dObjectForHrtf obj = { audioObject,
                Windows::Foundation::Numerics::float3(static_cast<float>(pos_dist(gen)), static_cast<float>(pos_dist(gen)), static_cast<float>(pos_dist(gen))),
                Windows::Foundation::Numerics::float3(static_cast<float>(vel_dist(gen)), static_cast<float>(vel_dist(gen)), static_cast<float>(vel_dist(gen))),
                static_cast<float>(static_cast<float>(yRotation_dist(gen))),
                static_cast<float>(static_cast<float>(deltaYRotation_dist(gen))),
                static_cast<float>(static_cast<float>(pitch_dist(gen))),
                format.nSamplesPerSec * 5 // 5 seconds of audio samples
            };

            objectVector.insert(objectVector.begin(), obj);
        }
    }
    spawnCounter++;

    // Loop through all dynamic audio objects
    std::vector<My3dObjectForHrtf>::iterator it = objectVector.begin();
    while (it != objectVector.end())
    {
        it->audioObject->GetBuffer(&buffer, &bufferLength);

        if (it->totalFrameCount >= frameCount)
        {
            // Write audio data to the buffer
            WriteToAudioObjectBuffer(reinterpret_cast<float*>(buffer), frameCount, it->frequency, format.nSamplesPerSec);

            // Update the position and volume of the audio object
            it->audioObject->SetPosition(it->position.x, it->position.y, it->position.z);
            it->position += it->velocity;


            Windows::Foundation::Numerics::float3 emitterDirection = Windows::Foundation::Numerics::float3(cos(it->yRotationRads), 0, sin(it->yRotationRads));
            Windows::Foundation::Numerics::float3 listenerDirection = Windows::Foundation::Numerics::float3(0, 0, 1);
            DirectX::XMFLOAT4X4 rotationMatrix;

            DirectX::XMMATRIX rotation = CalculateEmitterConeOrientationMatrix(emitterDirection, listenerDirection);
            XMStoreFloat4x4(&rotationMatrix, rotation);

            SpatialAudioHrtfOrientation orientation = {
                rotationMatrix._11, rotationMatrix._12, rotationMatrix._13,
                rotationMatrix._21, rotationMatrix._22, rotationMatrix._23,
                rotationMatrix._31, rotationMatrix._32, rotationMatrix._33
            };

            it->audioObject->SetOrientation(&orientation);
            it->yRotationRads += it->deltaYRotation;

            it->totalFrameCount -= frameCount;

            ++it;
        }
        else
        {
            // If the audio object reaches its lifetime, set EndOfStream and release the object

            // Write audio data to the buffer
            WriteToAudioObjectBuffer(reinterpret_cast<float*>(buffer), it->totalFrameCount, it->frequency, format.nSamplesPerSec);

            // Set end of stream for the last buffer 
            hr = it->audioObject->SetEndOfStream(it->totalFrameCount);

            it->audioObject = nullptr; // Release the object

            it->totalFrameCount = 0;

            it = objectVector.erase(it);
        }
    }

    // Let the audio-engine know that the object data are available for processing now
    hr = spatialAudioStreamForHrtf->EndUpdatingAudioObjects();

} while (objectVector.size() > 0);
When you are done rendering spatial audio, stop the spatial audio stream by calling ISpatialAudioRenderStreamForHrtf::Stop. If you are not going to use the stream again, free its resources by calling ISpatialAudioRenderStreamForHrtf::Reset.

C++

Copy
// Stop the stream 
hr = spatialAudioStreamForHrtf->Stop();

// We don't want to start again, so reset the stream to free it's resources.
hr = spatialAudioStreamForHrtf->Reset();

CloseHandle(bufferCompletionEvent);
The following code example shows the implementation of the CalculateEmitterConeOrientationMatrix helper method which was used in the example above to calculate the orientation matrix given the direction the 3D object is pointing.

C++

Copy
DirectX::XMMATRIX CalculateEmitterConeOrientationMatrix(Windows::Foundation::Numerics::float3 listenerOrientationFront, Windows::Foundation::Numerics::float3 emitterDirection)
{
    DirectX::XMVECTOR vListenerDirection = DirectX::XMLoadFloat3(&listenerOrientationFront);
    DirectX::XMVECTOR vEmitterDirection = DirectX::XMLoadFloat3(&emitterDirection);
    DirectX::XMVECTOR vCross = DirectX::XMVector3Cross(vListenerDirection, vEmitterDirection);
    DirectX::XMVECTOR vDot = DirectX::XMVector3Dot(vListenerDirection, vEmitterDirection);
    DirectX::XMVECTOR vAngle = DirectX::XMVectorACos(vDot);
    float angle = DirectX::XMVectorGetX(vAngle);

    // The angle must be non-zero
    if (fabsf(angle) > FLT_EPSILON)
    {
        // And less than PI
        if (fabsf(angle) < DirectX::XM_PI)
        {
            return DirectX::XMMatrixRotationAxis(vCross, angle);
        }

        // If equal to PI, find any other non-collinear vector to generate the perpendicular vector to rotate about
        else
        {
            DirectX::XMFLOAT3 vector = { 1.0f, 1.0f, 1.0f };
            if (listenerOrientationFront.x != 0.0f)
            {
                vector.x = -listenerOrientationFront.x;
            }
            else if (listenerOrientationFront.y != 0.0f)
            {
                vector.y = -listenerOrientationFront.y;
            }
            else // if (_listenerOrientationFront.z != 0.0f)
            {
                vector.z = -listenerOrientationFront.z;
            }
            DirectX::XMVECTOR vVector = DirectX::XMLoadFloat3(&vector);
            vVector = DirectX::XMVector3Normalize(vVector);
            vCross = DirectX::XMVector3Cross(vVector, vEmitterDirection);
            return DirectX::XMMatrixRotationAxis(vCross, angle);
        }
    }

    // If the angle is zero, use an identity matrix
    return DirectX::XMMatrixIdentity();
}